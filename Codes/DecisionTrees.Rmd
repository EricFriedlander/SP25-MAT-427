---
title: "Classification and Regression Trees (CART)"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)

library(tidyverse)
library(caret)
library(recipes)
```


#### ---------------------------------------------------------------------------
### Regression Tree: Ames Housing Dataset

```{r}
ames <- readRDS("AmesHousing.rds")   # load dataset
```

```{r}
# reorder levels of 'Overall_Qual'
ames$Overall_Qual <- factor(ames$Overall_Qual, levels = c("Very_Poor", "Poor", "Fair", "Below_Average", 
                                                          "Average", "Above_Average", "Good", "Very_Good", 
                                                          "Excellent", "Very_Excellent"))
```


```{r}
# split data

set.seed(050924)   # set seed

index <- createDataPartition(y = ames$Sale_Price, p = 0.7, list = FALSE)   # consider 70-30 split

ames_train <- ames[index,]   # training data

ames_test <- ames[-index,]   # test data
```


```{r}
# create recipe and blueprint, prepare and apply blueprint

set.seed(050924)   # set seed

ames_recipe <- recipe(Sale_Price ~ ., data = ames_train)   # set up recipe

blueprint <- ames_recipe %>%    
  step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %>%   # filter out zv/nzv predictors
  step_impute_mean(Gr_Liv_Area) %>%                                    # impute missing entries
  step_integer(Overall_Qual) %>%                                       # numeric conversion of levels of the predictors   
  step_center(all_numeric(), -all_outcomes()) %>%                      # center (subtract mean) all numeric predictors
  step_scale(all_numeric(), -all_outcomes()) %>%                       # scale (divide by standard deviation) all numeric predictors
  step_other(Neighborhood, threshold = 0.01, other = "other") %>%      # lumping required predictors
  step_dummy(all_nominal(), one_hot = FALSE)                           # one-hot/dummy encode nominal categorical predictors


prepare <- prep(blueprint, data = ames_train)    # estimate feature engineering parameters based on training data


baked_train <- bake(prepare, new_data = ames_train)   # apply the blueprint to training data

baked_test <- bake(prepare, new_data = ames_test)    # apply the blueprint to test data
```


```{r}
# implement CV to tune hyperparameter

set.seed(050924)   # set seed

cv_specs <- trainControl(method = "repeatedcv", number = 5, repeats = 5)   # CV specifications


library(rpart)   # for trees

tree_cv <- train(blueprint,
                 data = ames_train,
                 method = __________,  
                 trControl = cv_specs,
                 tuneLength = 20,                   # considers a grid of 20 possible tuning parameter values
                 metric = "RMSE")
```


```{r}
tree_cv$bestTune    # optimal hyperparameter
```


```{r}
min(tree_cv$results$RMSE)   # optimal CV RMSE
```


```{r}
# build optimal final model

final_model <- rpart(formula = __________, 
                     data = __________,                      
                     cp = __________,   
                     xval = 0,                # no further CV
                     method = __________)     # for regression trees
```


```{r}
# obtain predictions and test set RMSE

final_model_preds <- predict(object = __________, newdata = __________, type = __________)    # obtain predictions

sqrt(mean((final_model_preds - baked_test$Sale_Price)^2))   # calculate test set RMSE
```


```{r}
summary(final_model)    
```


```{r, fig.align='center', fig.height=6, fig.width=8}
library(rpart.plot)

rpart.plot(final_model)    
```


```{r, fig.align='center', fig.height=6, fig.width=8}
# variable importance

vip(object = tree_cv, num_features = 20, method = "model")          
```


#### ---------------------------------------------------------------------------
### Regression Tree (no pruning): Ames Housing Dataset

```{r}
# build optimal final model

final_model_no_prune <- rpart(formula = __________, 
                              data = __________,                      
                              cp = 0,                  # no pruning
                              xval = 0,                # no CV
                              method = __________)     # for regression trees
```


```{r}
# obtain predictions and test set RMSE

final_model_no_prune_preds <- predict(object = __________, newdata = __________, type = __________)    # obtain predictions

sqrt(mean((final_model_no_prune_preds - baked_test$Sale_Price)^2))   # calculate test set RMSE
```


```{r}
summary(final_model_no_prune)    
```


```{r, fig.align='center', fig.height=6, fig.width=8}
rpart.plot(final_model_no_prune)    
```


#### ---------------------------------------------------------------------------
### Regression Tree (minimal feature engineering): Ames Housing Dataset

```{r}
# create new blueprint (minimal feature engineering), prepare and apply blueprint

set.seed(050924)   # set seed

ames_recipe <- recipe(Sale_Price ~ ., data = ames_train)   # set up recipe

blueprint_minimal <- ames_recipe %>%    
  step_impute_mean(Gr_Liv_Area)       # impute missing entries                     


prepare_minimal <- prep(blueprint_minimal, data = ames_train)    # estimate feature engineering parameters based on training data


baked_train_minimal <- bake(prepare_minimal, new_data = ames_train)   # apply the blueprint to training data

baked_test_minimal <- bake(prepare_minimal, new_data = ames_test)    # apply the blueprint to test data
```


```{r}
# implement CV to tune hyperparameters

set.seed(050924)   # set seed

cv_specs <- trainControl(method = "repeatedcv", number = 5, repeats = 5)   # CV specifications


library(rpart)

tree_cv_min_fe <- train(blueprint_minimal,
                        data = ames_train,
                        method = __________,  
                        trControl = cv_specs,
                        tuneLength = 20,                   # considers a grid of 20 possible tuning parameter values
                        metric = "RMSE")
```


```{r}
tree_cv_min_fe$bestTune    # optimal hyperparameters
```


```{r}
min(tree_cv_min_fe$results$RMSE)   # optimal CV RMSE
```


```{r, fig.align='center', fig.height=6, fig.width=8}
# variable importance

vip(object = tree_cv_min_fe, num_features = 20, method = "model")          
```


#### ---------------------------------------------------------------------------