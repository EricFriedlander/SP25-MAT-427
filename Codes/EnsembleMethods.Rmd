---
title: "Ensemble Methods"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)

library(tidyverse)
library(caret)
library(recipes)
library(rpart)
library(rpart.plot)
```


#### ---------------------------------------------------------------------------
### Bagging: Ames Housing Dataset

```{r}
ames <- readRDS("AmesHousing.rds")   # load dataset
```

```{r}
# reorder levels of 'Overall_Qual'
ames$Overall_Qual <- factor(ames$Overall_Qual, levels = c("Very_Poor", "Poor", "Fair", "Below_Average", 
                                                          "Average", "Above_Average", "Good", "Very_Good", 
                                                          "Excellent", "Very_Excellent"))
```


```{r}
# split data

set.seed(051423)   # set seed

train_index <- createDataPartition(y = ames$Sale_Price, p = 0.7, list = FALSE)   # consider 70-30 split

ames_train <- ames[train_index,]   # training data

ames_test <- ames[-train_index,]   # test data
```


```{r}
# create recipe and blueprint, prepare and apply blueprint

set.seed(051423)   # set seed

ames_recipe <- recipe(Sale_Price ~ ., data = ames_train)   # set up recipe

blueprint <- ames_recipe %>%    
  step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %>%   # filter out zv/nzv predictors
  step_impute_mean(Gr_Liv_Area) %>%                                    # impute missing entries
  step_integer(Overall_Qual) %>%                                       # numeric conversion of levels of the predictors   
  step_center(all_numeric(), -all_outcomes()) %>%                      # center (subtract mean) all numeric predictors
  step_scale(all_numeric(), -all_outcomes()) %>%                       # scale (divide by standard deviation) all numeric predictors
  step_other(Neighborhood, threshold = 0.01, other = "other") %>%      # lumping required predictors
  step_dummy(all_nominal(), one_hot = FALSE)                           # one-hot/dummy encode nominal categorical predictors


prepare <- prep(blueprint, data = ames_train)    # estimate feature engineering parameters based on training data


baked_train <- bake(prepare, new_data = ames_train)   # apply the blueprint to training data

baked_test <- bake(prepare, new_data = ames_test)    # apply the blueprint to test data
```


```{r}
set.seed(051423)   # set seed

library(ipred)   # for bagging

bag_fit <- bagging(formula = __________, 
                   data = __________,
                   nbagg = 500,                             # number of trees to grow (bootstrap samples) usually 500
                   coob = TRUE,                             # yes to computing OOB error estimate
                   control = rpart.control(minsplit = 2,    # split a node if at least 2 observations present
                                           cp = 0,          # no pruning (let the trees grow tall)
                                           xval = 0))       # no CV 
```


```{r}
bag_fit   # results of bagging 
```


```{r}
bag_fit$err   # OOB RMSE estimate
```



```{r, eval=FALSE}
set.seed(051424)   # set seed

cv_specs <- trainControl(method = "repeatedcv", number = 5, repeats = 1)   # CV specifications

library(ipred)
library(e1071)

bagging_cv <- train(blueprint,
                    data = ames_train,
                    method = "treebag",  
                    trControl = cv_specs,
                    nbagg = 500,                   
                    control = rpart.control(minsplit = 2, cp = 0),    
                    metric = "RMSE")
```


```{r}
# obtain predictions on the test set

final_model_preds <- predict(object = __________, newdata = __________)     # use 'type = "class"' for classification trees

sqrt(mean((final_model_preds - baked_test$Sale_Price)^2))   # test set RMSE
```


```{r}
# variable importance

imp <- varImp(bag_fit)      # look at the object created
```




#### ---------------------------------------------------------------------------
### Random Forests: Ames Housing Dataset

```{r}
# CV with random forests

set.seed(051423)   # set seed

cv_specs <- trainControl(method = "cv", number = 5)   # CV specifications


library(ranger)
library(e1071)

param_grid <- expand.grid(mtry = seq(1, 30, 1),     # sequence of 1 to at least half the number of predictors
                          splitrule = __________,   # use "gini" for classification
                          min.node.size = 2)        # for each tree


rf_cv <- train(blueprint,
               data = ames_train,
               method = __________,
               trControl = cv_specs,
               tuneGrid = param_grid,
               metric = "RMSE")
```


```{r}
rf_cv$bestTune$mtry   # optimal tuning parameter
```


```{r}
min(rf_cv$results$RMSE)   # optimal CV RMSE
```


```{r}
# fit final model

final_model <- ranger(formula = __________,
                      data = __________,
                      num.trees = __________,
                      mtry = __________,
                      splitrule = __________,
                      min.node.size = __________, 
                      importance = "impurity")
```


```{r}
# obtain predictions on the test set

final_model_preds <- predict(object = __________, data = __________, type = "response")  # predictions on test set

sqrt(mean((final_model_preds$predictions - baked_test$Sale_Price)^2))  # test set RMSE
```


```{r}
# variable importance

head(sort(final_model$variable.importance, decreasing = TRUE), 10)      # top 10 most important features
```




#### ---------------------------------------------------------------------------
