---
title: "Machine Learning - Homework 7"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
```


### ----------------------------------------------------------------------------

# Instructions

**Since this is going to be peer-reviewed anonymously, please don't indicate your name on the assignment or the file name.** 


# Objectives

* Practice with the overall ML process

* Practice with remedying class imbalances

* Practice with regularization



### ----------------------------------------------------------------------------

## Question 1

This problem concerns heart disease diagnosis based on certain characteristics of patients. You will work with the **HeartDisease.rds** dataset. Load the dataset into your R session using the following code.

```{r}
hd <- readRDS("HeartDisease.rds")
```

The dataset contains information on the following 14 variables.

* `age` - age in years
* `sex` - 'male' or 'female'
* `cp` - chest pain type (1: typical angina; 2: atypical angina; 3: non-anginal pain; 4: asymptomatic)
* `trestbps` - resting blood pressure (in mm Hg on admission to the hospital)
* `chol` - serum cholesterol in mg/dl
* `fbs` - Is fasting blood sugar > 120 mg/dl? ('TRUE' or 'FALSE')
* `restecg` - resting electrocardiographic results (0, 1 or, 2)
* `thalach` - maximum heart rate achieved
* `exang` - exercise induced angina ('yes' or 'no')
* `oldpeak` - ST depression induced by exercise relative to rest
* `slope` - the slope of the peak exercise ST segment (1: upsloping; 2: flat; 3: downsloping)
* `ca` - number of major vessels (0-3) colored by fluoroscopy
* `thal` - one of 'normal', 'fixed defect', or 'reversible defect'
* `target` - 'Yes' (presence of heart disease) or 'No' (absence of heart disease)

**Consider `target` as the response variable and the rest of the variables as predictors.**

The response is fairly balanced as can be seen below, so remedying is not necessary for this data.

```{r}
table(hd$target)    # frequency counts

prop.table(table(hd$target))   # relative frequencies
```

Let's relevel the response below.

```{r}
levels(hd$target)   # original levels

hd$target <- factor(hd$target, levels = c("Yes", "No"))   # relevel
levels(hd$target)   # updated levels
```


The objective is to compare the performance (in terms of **Sensitivity (Recall)**) of the following two models:

* Logistic regression;

* $K$-NN classifier with optimal $K$ chosen by CV. Consider the grid of possible $K$ values given below.


**Perform the following tasks.**

* Investigate the dataset and complete any necessary tasks. You can refer to the variable descriptions for some of your choices.

```{r}
# write your R code here
set.seed(2087)
library(caret)
library(recipes)



```


* Split the data into training and test sets (consider a 70-30 split).

```{r}
# write your R code here
set.seed(2087)
library(caret)
library(recipes)



```


* Perform required data preprocessing and create the blueprint. If using `step_dummy()`, set `one_hot = FALSE`. Be careful when imputing missing entries for features (refer to **slide 19 from week 5** for possible imputation options). Prepare the blueprint on the training data. Obtain the modified training and test datasets. **Explain the blueprint steps that you use.**

```{r}
# write your R code here
set.seed(2087)
library(caret)
library(recipes)



```

```{}
# write your answer here



```


* Implement 5-fold CV (1 repeat) for each of the models above. 

```{r}
# write your R code here
set.seed(2087)
library(caret)
library(recipes)






k_grid <- expand.grid(k = seq(1, 25, by = 3))   # for KNN





```


* Report the optimal CV Sensitivity of each model. Report the the optimal value of $K$ for the $K$-NN classifier. Which model performs better in this situation? 

```{r}
# write your R code here
set.seed(2087)
library(caret)
library(recipes)


```

```{}
# write your answer here



```


* Finally, with the optimal model, obtain class label predictions on the test set (use default threshold of 0.5). Create the corresponding confusion matrix and report the test set sensitivity.

**Hint: To specify all predictors in the model , use `target ~ .`**

```{r}
# write your R code here
set.seed(2087)
library(caret)
library(recipes)



```




### ----------------------------------------------------------------------------

## Question 2

In this question, you will work with the **Default** dataset. Load the dataset into your R session using the following code.

```{r}
library(ISLR2)

data("Default")
```

The dataset contains information on 10,000 people on their 

* credit card default status - variable `default`
* student status - variable `student`
* credit card balance - variable `balance`, and
* income - variable `income`

We are interested in predicting `default` given the other variables. As can be seen below, the response is severely imbalanced.

```{r}
table(Default$default)   # frequency counts

prop.table(table(Default$default))   # relative frequencies
```


We will consider 'Yes' to be the positive class, that is, we are interested in predicting more accurately people who will default rather than who will not. Let's relevel the response variable below.

```{r}
levels(Default$default)   # original levels

Default$default <- factor(Default$default, levels = c("Yes", "No"))   # relevel
levels(Default$default)   # updated levels
```

Consider the following two classifiers.

* Logistic regression;

* $K$-NN with optimal $K$ chosen by CV. Consider the grid of possible $K$ values to be integers from $1, 2, \ldots, 10$.


Use the following strategies we learned in class to come up with the best possible model in terms of the sensitivity.

* tuning models according to sensitivity;

* choosing alternative cutoffs;

* either upsampling or downsampling.


The sensitivity of the baseline classifier (assuming balanced data with a threshold of 0.5) is around 35%, with an overall accuracy of 97% and specificity of approximately 98%. 

**Use appropriate blueprint for feature engineering. Report which remedy lead to the best model. If you find an alternative cutoff other than 0.5, report that as well. Report which model yields an optimal result in each suggested remedy. Finally, report the test set sensitivity using the optimal model.**


Copy and paste the following code chunk as many times as you want to go through the process.

```{r}
# write your R code here
set.seed(2087)
library(caret)
library(recipes)



```

```{}
# write your answer here



```




### ----------------------------------------------------------------------------

## Question 3 (TO BE GRADED)

In this question, you will work with the **Hitters.rds** dataset. Load the dataset into your R session using the following code.

```{r}
Hitters <- readRDS("Hitters.rds")   # load dataset
```


The dataset contains baseball statistics from the 1986 and 1987 seasons. The task is to predict `Salary` using the rest of the variables in the dataset. Compare the performance (in terms of **RMSE**) of the following two models:

* A linear regression model;

* A LASSO model chosen by CV. Consider the grid of possible $\lambda$ values given below.


**Perform the following tasks.**

* Investigate the dataset and complete any necessary tasks.

```{r}
# write your R code here
set.seed(2087)
library(caret)
library(recipes)



```


* Split the data into training and test sets (80-20).

```{r}
# write your R code here
set.seed(2087)
library(caret)
library(recipes)



```


* Perform required data preprocessing and create the blueprint. If using `step_dummy()`, set `one_hot = FALSE`. Prepare the blueprint on the training data. Obtain the modified training and test datasets. **Explain the blueprint steps that you use.**

```{r}
# write your R code here
set.seed(2087)
library(caret)
library(recipes)



```

```{}
# write your answer here



```


* Implement 5-fold CV repeated 5 times for each of the models above.

```{r}
# write your R code here
set.seed(2087)
library(caret)
library(recipes)
library(glmnet)






lambda_grid <- 10^seq(-2, 2, length = 100)    # for LASSO






```


* Report the optimal CV RMSE of each model. Report the optimal value of $\lambda$ for the LASSO model. Which model performs better in this situation?

```{r}
# write your R code here
set.seed(2087)
library(caret)
library(recipes)



```

```{}
# write your answer here



```


* Using the optimal model, obtain predictions on the test set. Calculate and report the test set RMSE.

```{r}
# write your R code here
set.seed(2087)
library(caret)
library(recipes)
library(glmnet)



```


* Using the final model, obtain variable importance measures for the features. List the three most important features. Also, list the features whose coefficients have been shrunk exactly to zero.

```{r}
# write your R code here
set.seed(2087)
library(caret)
library(recipes)
library(vip)



```

```{}
# write your answer here



```




### ----------------------------------------------------------------------------

## Question 4

Suppose we estimate the coefficients $\left(\beta\text{'s}\right)$ of a linear regression model by regularization, that is, by minimizing

$$\displaystyle \sum_{i=1}^{n} \bigg(y_i - \left(\beta_0 + \beta_1 \ x_{i1} + \beta_2 \ x_{i2} + \ldots + \beta_p \ x_{ip}\right) \bigg)^2 + \lambda \sum_{j=1}^{p} \beta_j^2$$
where $\lambda$ acts as the tuning parameter. 

For parts (a) through (e), indicate which of options (i) through (v) is correct. **Justify your answer.**

(a) As we increase $\lambda$ from 0, the training error will:

i. Increase initially, and then eventually start decreasing in an inverted U shape.
ii. Decrease initially, and then eventually start increasing in a U shape.
iii. Steadily increase.
iv. Steadily decrease.
v. Remain constant.

(b) Repeat (a) for test error.

(c) Repeat (a) for variance.

(d) Repeat (a) for (squared) bias.

(e) Repeat (a) for the irreducible error.


```{}
# write your answer here












```




### ----------------------------------------------------------------------------
