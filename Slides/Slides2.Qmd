---
title: 'MATH 427: Machine Learning'
author: Eric Friedlander <br> [Much of the content in these slides have been adapted from Abhishek Chakraborty at Lawrence University]{.smaller}
format: revealjs
execute:
  echo: true
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

```{r setup}
#| include: false
library(tidyverse)
# library(ggformula)
# library(gridExtra)
library(ISLR2)
library(GGally)
library(gridExtra)
library(kableExtra)
library(plotly)
```

## Data Generating Process

Suppose we have

-   Features: $\mathbf{X}$
-   Target: $Y$
-   Goal: Predict $Y$ using $\mathbf{X}$

. . .

-   **Data generating process**: underlying, unseen and unknowable
    process that generates $Y$ given $\mathbf{X}$

## Population

More mathematically, the "true"/population model can be represented by

$$Y=f(\mathbf{X}) + \epsilon$$

where $\epsilon$ is a **random** error term (includes measurement error,
other discrepancies) independent of $\mathbf{X}$ and has mean zero.

. . .

*GOAL*: Estimate $f$

## [Why Estimate $f(\mathbf{X})$?]{.r-fit-text} {.smaller}

We wish to know about $f(\mathbf{X})$ for two reasons:

1.  Prediction: make an educated guess for what $y$ should be given a
    new $x_0$:
    $$\hat{y}_0=\hat{f}(x_0) \ \ \ \text{or} \ \ \ \hat{y}_0=\hat{C}(x_0)$$
2.  Inference: Understand the relationship between $\mathbf{X}$ and $Y$.

. . .

-   An ML algorithm that is developed mainly for predictive purposes is
    often termed as a **Black Box** algorithm.

## Prediction {.smaller}

There are two types of prediction problems:

-   **Regression** (response $Y$ is quantitative): Build a model
    $\hat{Y} = \hat{f}(\mathbf{X})$
-   **Classification** (response $Y$ is qualitative/categorical): Build
    a classifier $\hat{Y}=\hat{C}(\mathbf{X})$

. . .

-   Note: a "hat", $\hat{\phantom{f}}$, over an object represents an
    estimate of that object
    -   E.g. $\hat{Y}$ is an estimate of $Y$ and $\hat{f}$ is an
        estimate of $f$

## [Prediction and Inference]{.r-fit-text}

**Income dataset**

![Why ML? (from ISLR2)](images/2_2-1.png){.r-stretch}

## [Prediction and Inference]{.r-fit-text}

**Income dataset**

::: {layout-ncol="2"}
![](images/2_3-1.png)

![](images/2_4-1.png)

Why ML? (from ISLR2)
:::

## [Question!!!]{style="color:blue"} {.smaller}

Based on the previous two slides, which of the following statements are
correct?

::: panel-tabset
## Questions

1.  As `Years of Education` increases, `Income` increases, keeping
    `Seniority` fixed.
2.  As `Years of Education` increases, `Income` decreases, keeping
    `Seniority` fixed.
3.  As `Years of Education` increases, `Income` increases.
4.  As `Seniority` increases, `Income` increases, keeping
    `Years of Education` fixed.
5.  As `Seniority` increases, `Income` decreases, keeping
    `Years of Education` fixed.
6.  As `Seniority` increases, `Income` increases.

## Answers

1.  As `Years of Education` increases, `Income` increases, keeping
    `Seniority` fixed. **TRUE**
2.  As `Years of Education` increases, `Income` decreases, keeping
    `Seniority` fixed. **FALSE**
3.  As `Years of Education` increases, `Income` increases. **TRUE**
4.  As `Seniority` increases, `Income` increases, keeping
    `Years of Education` fixed. **TRUE**
5.  As `Seniority` increases, `Income` decreases, keeping
    `Years of Education` fixed. **FALSE**
6.  As `Seniority` increases, `Income` increases. **TRUE**
:::

## Discussion

What's the difference between these two statements:

1.  As `Years of Education` increases, `Income` increases, keeping
    `Seniority` fixed.
2.  As `Years of Education` increases, `Income` increases.

<!-- ## [Question!!!]{style="color:blue"} {.smaller} -->

<!-- Which of the following statements are correct? -->

<!-- 1. The increase in `Income` resulting from increase in `Years of Education` keeping other variables fixed is **more** than the increase in `Income` resulting from increase in `Seniority` keeping other variables fixed. -->

<!-- 2. The increase in `Income` resulting from increase in `Years of Education` keeping other variables fixed is **less** than the increase in `Income` resulting from increase in `Seniority` keeping other variables fixed. -->

## [How Do We Estimate $f(\mathbf{X})$?]{.r-fit-text}

Broadly speaking, we have two approaches.

1.  Parametric methods
2.  Non-parametrics methods

## Parametric Methods

-   Assume a functional form for $f(\mathbf{X})$
    -   Linear Regression:
        $f(\mathbf{X})=\beta_0 + \beta_1 \mathbf{x}_1 + \beta_2 \mathbf{x}_2 + \ldots + \beta_p \mathbf{x}_p$
    -   Estimate the parameters $\beta_0, \beta_1, \ldots, \beta_p$
        using labeled data
-   Choosing $\beta$'s that minimize some error metrics is called
    **fitting** the model
-   The data we use to fit the model is called our **training data**

## Parametric Methods {.smaller}

![Parametric model fit (from ISLR2)](images/2_2-1.png){.r-stretch}

::: incremental
-   What are some potential parametric models that could result in this
    picture?
-   Right: actually the true relationship
:::

## Parametric Methods {.smaller}

**Income dataset**

::: {layout-ncol="2"}
![True relationship](images/2_3-1.png){width="60%"}

![Parametric model](images/2_4-1.png){width="60%"}

From ISLR2
:::

::: incremental
-   What are some functions that could have resulted in the model on the
    right?
-   $\text{Income} \approx \beta_0 + \beta_1\times\text{Years of Education} + \beta_2\times\text{Seniority}$
:::

## Non-parametric Methods {.smaller}

-   Non-parametric approach: no explicit assumptions about the
    functional form of $f(\mathbf{X})$
-   Much more observations (compared to a parametric approach) required
    to fit non-parametric model
    -   **Idea:** parametric model restricts space of possible answers

**Income dataset**

::: {layout-ncol="2"}
![True relationship](images/2_3-1.png){width="50%"}

![Non-parametric model fit](images/2_5-1.png){width="50%"}

From ISLR2
:::

## [Supervised Learning: Flexibility of Models]{.r-fit-text} {.smaller}

-   Flexibility: smoothness of functions
-   More theoretically: how many parameters are there to estimate?

```{r}
#| echo: FALSE
#| r-stretch: TRUE
#| fig-align: 'center'

set.seed(208)

# simulate data

x <- runif(n = 100, min = 20, max = 40)   # input/predictor

e <- rnorm(n = 100, mean = 0, sd = 1)  # error

a <- 3

b <- 0.87

c <- 0.5

fx <- a + (b * sqrt(x)) + (c * sin(x))   # true function

y <- fx + e    # observed responses

toy_data <- data.frame(input = x, true_form = fx, response = y)   # create data frame to store values


# plot linear model (red) and non-linear model (blue) 
g3 <- ggplot(data = toy_data, aes(x = input, y = response)) + 
  geom_point() + 
  geom_function(fun = function(x) a+(b*sqrt(x))+(c*sin(x)), aes(color = "true model"), linewidth = 1.5) + 
  geom_smooth(method = "lm", se = FALSE, aes(color = "linear model")) + 
  geom_smooth(formula = y ~ sqrt(x) + sin(x), se = FALSE, aes(color = "non-linear model")) +
  scale_color_manual(values = c("true model" = "red", "linear model" = "blue", "non-linear model" = "green")) + 
  theme(legend.title = element_blank(),
       text = element_text(size=20)) +
  labs(title = "Comparing two models", y = "y", x = "x")

g3

```

[More flexible $\implies$ More complex $\implies$ Less Smooth $\implies$
Less Restrictive $\implies$ Less Interpretable]{.r-fit-text}

## Supervised Learning: Some Trade-offs {.smaller}

-   Prediction Accuracy versus Interpretability
-   Good Fit versus Over-fit or Under-fit

![Trade-off between flexibility and interpretability (from
ISLR2)](images/2_7-1-01.png)

## [Supervised Learning: Selecting a Model]{.r-fit-text} {.smaller}

-   Why so many different ML techniques?
-   **There is no free lunch in statistics**: All methods have different
    pros and cons
    -   Must select correct model for each use-case
-   Relevant questions in model selection:
    -   How much observations $n$ and variables $p$?
    -   How important is prediction relative to interpretability?
    -   Do we expect relationship to be non-linear?
    -   Regression or classification?

## [Supervised Learning: Assessing Model Performance]{.r-fit-text} {.smaller}

-   When we estimate $f(\mathbf{X})$ using $\hat{f}(\mathbf{X})$, then,

$$\underbrace{E\left[Y-\hat{Y}\right]^2}_{Error}=E\left[f(\mathbf{X})+\epsilon - \hat{f}(\mathbf{X})\right]^2=\underbrace{\left[f(\mathbf{X})-\hat{f}(\mathbf{X})\right]^2}_{Reducible} + \underbrace{Var(\epsilon)}_{Irreducible}$$

-   $E\left[Y-\hat{Y}\right]^2$: Expected (average) squared difference
    between predicted and actual (observed) response, **Mean Squared Error (MSE)**
-   Goal: find an estimate of $f(\mathbf{X})$ to minimize the reducible
    error

## [Supervised Learning: Assessing Model Performance]{.r-fit-text} {.smaller}

::: {style="font-size: 75%;"}
-   Labeled training data $(x_1,y_1), (x_2, y_2), \ldots, (x_n,y_n)$
    -   i.e. $n$ training observations
-   Fit/train a model training data
    -   $\hat{y}=\hat{f}(x)$, regression
    -   $\hat{y}=\hat{C}(x)$, classification\
-   Obtain estimates $\hat{f}(x_1), \hat{f}(x_2), \ldots, \hat{f}(x_n)$
    (or, $\hat{C}(x_1), \hat{C}(x_2), \ldots, \hat{C}(x_n)$) of training
    data
-   Compute error:
    -   **Regression**
        $$\text{Training MSE}=\text{Average}_{Training} \left(y-\hat{f}(x)\right)^2 = \frac{1}{n} \displaystyle \sum_{i=1}^{n} \left(y_i-\hat{f}(x_i)\right)^2$$
    -   **Classification** $$
        \begin{aligned}
        \text{Training Error Rate}
        &=\text{Average}_{Training} \ \left[I \left(y\ne\hat{C}(x)\right) \right]\\
        &= \frac{1}{n} \displaystyle \sum_{i=1}^{n} \ I\left(y_i \ne \hat{C}(x_i)\right)
        \end{aligned}
        $$
:::

## [Supervised Learning: Assessing Model Performance]{.r-fit-text} {.smaller}

-   In general, not interested in performance on training data
-   Want: performance on unseen test data... why?
-   Fresh test data:
    $(x_1^{test},y_1^{test}), (x_2^{test},y_2^{test}), \ldots, (x_m^{test},y_m^{test})$. -
    Compute test error:
    -   **Regression**
        $$\text{Test MSE}=\text{Average}_{Test} \left(y-\hat{f}(x)\right)^2 = \frac{1}{m} \displaystyle \sum_{i=1}^{m} \left(y_i^{test}-\hat{f}(x_i^{test})\right)^2$$
    -   **Classification**
        $$\text{Test Error Rate}=\text{Average}_{Test} \ \left[I \left(y\ne\hat{C}(x)\right) \right]= \frac{1}{m} \displaystyle \sum_{i=1}^{m} \ I\left(y_i^{test} \ne \hat{C}(x_i^{test})\right)$$

## [Supervised Learning: Bias-Variance Trade-off]{.r-fit-text} {.smaller}

-   Model fit on training data $\hat{f}(x)$
-   "True" relationship: $Y=f(x)+\epsilon$
-   $(x_0^{test}, y_0^{test})$: test observation
-   Bias-Variance Trade-Off (Theorical)
    $$\underbrace{E\left(y_0^{test}-\hat{f}(x_0^{test})\right)^2}_{total \ error}=\underbrace{Var\left(\hat{f}(x_0^{test})\right)}_{source \ 1} + \underbrace{\left[Bias\left(\hat{f}(x_0^{test})\right)\right]^2}_{source \ 2}+\underbrace{Var(\epsilon)}_{source \ 3}$$
    where
    $Bias\left(\hat{f}(x_0)\right)=E\left(\hat{f}(x_0)\right)-f(x_0)$

. . .

-   Question: Where is $\hat{y}_0^{test}$?

## [Supervised Learning: Bias-Variance Trade-off]{.r-fit-text}

-   Reducible Error:
    -   Source 1: how $\hat{f}(x)$ varies among different randomly
        selected possible training data (**Variance**)
    -   Source 2: how $\hat{f}(x)$ (when predicting the test data)
        differs from its target $f(x)$ (**Bias**)
-   Irreducible Error:
    -   Source 3: how $y$ differs from "true" $f(x)$

## [Supervised Learning: Comparing Bias and Variance]{.r-fit-text}

Insert App Stuff Here

## Bias-Variance Trade-off: Example {.smaller}

-   For now: focus on regression problems (ideas extend to
    classification)
-   Consider: three different examples of simulated "toy" datasets and
    three types of models ($\hat{f}_i(.)$)
    +   Linear Regression [**orange**]{style="color:orange"}
    +   Smoothing Spline 1 [**blue**]{style="color:cornflowerblue"}
    +   More flexible Smoothing Spline 2
        [**green**]{style="color:green"}
-   "True" (simulated) function $f(.)$ [**black**]{style="color:black"}
-   [**Training Error**]{style="color:grey"}
-   [**Test Error**]{style="color:red"}

## Bias-Variance Trade-off: Example

![From ISLR2](images/2_9-1.png)

## Bias-Variance Trade-off: Example

![From ISLR2](images/2_10-1.png)

## Bias-Variance Trade-off: Example

![From ISLR2](images/2_11-1.png)

## Bias-Variance Trade-off: Example

![From ISLR2](images/2_12-1.png)

## [Question!!!]{style="color:blue"}

As flexibility increases,

::: panel-tabset

## Questions

1. its variance (increases/decreases)
2. its bias  (increases/decreases)
3. its training MSE  (increases/decreases)
4. its test MSE  (describe)

## Answers
1. its variance  (**increases**)
2. its bias  (**decreases**)
3. its training MSE  (**decreases**)
4. its test MSE  (**decreases at first, then increases and the model starts to overfit, U-shaped**)

:::

## A Familiar Supervised Learning Model {.smaller}

- Assume relationship between $\mathbf{X}$ and $Y$ is:
$$Y=f(\mathbf{X}) + \epsilon$$
where $\epsilon$ is a **random** error term (includes measurement error, other discrepancies) independent of $\mathbf{X}$ and has mean zero.
- **Objective**: To approximate/estimate $f(\mathbf{X})$
- **Linear Regression**: assume that $f(\mathbf{X})$ is a linear function of $\mathbf{X}$
  + For $p=1$: $f(\mathbf{X}) = \beta_0 + \beta_1 X_1$
  + For $p > 1$: $f(\mathbf{X}) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p$

## Linear Regression {.smaller}

Suppose the CEO of a restaurant franchise is considering opening new outlets in different cities. They would like to expand their business to cities that give them higher profits with the assumption that highly populated cities will probably yield higher profits.

They have data on the population (in 100,000) and profit (in $1,000) at 97 cities where they currently have outlets.

```{r}
outlets <- readRDS("outlets.rds")   # load dataset
head(outlets, 10) # first ten observations of the dataset
```

## Linear Regression

- **Objective**: choose "best" $\beta_0$ and $\beta_1$ if we assume
$$\text{profit} = \beta_0 + \beta_1\times\text{population}$$
- Once this is done, we can:
  + predict the profit for a new city with a given population
  + understand the relationship between `population` and `profit` better

## Outlet EDA

```{r}
ggpairs(data = outlets)
```

## [Linear Regression: Estimating Parameters]{.r-fit-text} {.smaller}

::: {.incremental}
- Suppose we have $\hat{\beta}_0$ and $\hat{\beta}_1$
- Observed response: $y_i$ for $i=1,\ldots,n$
- Predicted response: $\hat{y}_i$ for $i=1, \ldots, n$
- Residual: $e_i = \hat{y}_i - y_i$ for $i=1, \ldots, n$
- **Mean Squared Error (MSE)**: $MSE =\dfrac{e^2_1+e^2_2+\ldots+e^2_n}{n}$  also known as the **loss/cost function**
- **GOAL**: Find $\hat{\beta}_0$ and $\hat{\beta}_1$ which minimizes $MSE$
:::


## Gradient Descent Algorithm

- How do we minimize the $MSE$?
  + Can be done "analytically" but most ML algorithms can't be fit that way
- **Today**: popular optimization algorithm called **gradient descent**.
- **NOTE**: Gradient Descent is not a machine learning technique. It is an optimization technique that helps to fit machine learning models.

## Gradient Descent Algorithm {.smaller}

- Think of the MSE as a function of $\hat{\beta}_0$ and $\hat{\beta}_1$
  + $\text{MSE} = \frac{\sum (y_i - \hat{\beta}_0 - \hat{\beta}_1)}{n}$

<!-- ```{r} -->
<!-- #| echo: FALSE -->

<!-- centerb0 <- -4 -->
<!-- centerb1 <- 1 -->
<!-- grid <-  expand.grid(b0 = seq(from = -50 + centerb0, to = 50 + centerb0, by = 1), -->
<!--                       b1 = seq(from = -100 + centerb1, to = 100 + centerb1, by = 1)) -->

<!-- plot_axes <- tibble(b0 = seq(from = -50 + centerb0, to = 50 + centerb0, by = 1/2), -->
<!--                       b1 = seq(from = -100 + centerb1, to = 100 + centerb1, by = 1)) -->
<!-- grid <- tibble(grid) |>  -->
<!--   rowwise() |>  -->
<!--   mutate(MSE = sum((outlets$profit - b0 - b1*outlets$population)^2))  -->
<!-- grid_mat <- xtabs(MSE ~ b0 + b1, data = grid) -->

<!-- plot_ly(plot_axes, x = ~b0, y = ~b1) |>  -->
<!--   add_surface(z = ~grid_mat) -->
<!-- ``` -->

## Gradient Descent Algorithm {.smaller}

```{r}
#| echo: FALSE

# create data to plot

x <- seq(-5, 5, by = .05)

y <- x^2 + 3

df <- data.frame(x, y)

step <- 5

step_size <- .2

for(i in seq_len(18)) {

  next_step <- max(step) + round(diff(range(max(step), which.min(df$y))) * step_size, 0)

  step <- c(step, next_step)

  next

}

steps <- df[step, ] %>%

  mutate(x2 = lag(x), y2 = lag(y)) %>%

  dplyr::slice(1:18)

# plot

ggplot(df, aes(x, y)) +
  geom_line(linewidth = 1.5, alpha = .5) +
  geom_point(aes(x=steps$x[1], y = steps$y[1]), size = 3, shape = 21, fill = "blue", alpha = .5) +
  theme_classic() +
  scale_y_continuous("Cost/Loss function (MSE)", limits = c(0, 30)) +
  xlab("parameter to estimate")

```

Updates to the variable:
$$
\begin{aligned}
\text{new value of variable} &= \text{old value of variable}\\
&\qquad- \text{step size} \times \text{gradient of function with respect to variable}
\end{aligned}
$$

## Gradient Descent Algorithm

```{r}
#| echo: FALSE

# plot

ggplot(df, aes(x, y)) +

  geom_line(linewidth = 1.5, alpha = .5) +

  theme_classic() +

  scale_y_continuous("Cost/Loss function (MSE)", limits = c(0, 30)) +

  xlab("parameter to estiamte") +

  geom_segment(data = df[c(5, which.min(df$y)), ], aes(x = x, y = y, xend = x, yend = -Inf), lty = "dashed") +

  geom_point(data = filter(df, y == min(y)), aes(x, y), size = 4, shape = 21, fill = "yellow") +

  geom_point(data = steps, aes(x, y), size = 3, shape = 21, fill = "blue", alpha = .5) +

  geom_curve(data = steps[-1,], aes(x = x, y = y, xend = x2, yend = y2), curvature = 1, lty = "dotted") +

  theme(

    axis.ticks = element_blank(),

    axis.text = element_blank()

  ) +

  annotate("text", x = df[5, "x"], y = 1, label = "initial value", hjust = -0.1, vjust = .8) +

  annotate("text", x = df[which.min(df$y), "x"], y = 1, label = "minimium", hjust = -0.1, vjust = .8) +

  annotate("text", x = df[5, "x"], y = df[5, "y"], label = "learning rate (step size)", hjust = -.8, vjust = 0)

```

## Gradient Descent Algorithm {.smaller}

```{r}
#| echo: FALSE
#| layout-ncol: 2
#| fig-subcap: 
#|   - "Step size too big"
#|   - "Step size too small"


# create too small of a learning rate

step <- 5

step_size <- .05

for(i in seq_len(10)) {

  next_step <- max(step) + round(diff(range(max(step), which.min(df$y))) * step_size, 0)

  step <- c(step, next_step)

  next

}

too_small <- df[step, ] %>%

  mutate(x2 = lag(x), y2 = lag(y))

# plot

ggplot(df, aes(x, y)) +

  geom_line(size = 1.5, alpha = .5) +

  theme_classic() +

  scale_y_continuous("Cost/Loss function of the variable", limits = c(0, 30)) +

  xlab("variable") +

  geom_segment(data = too_small[1, ], aes(x = x, y = y, xend = x, yend = -Inf), lty = "dashed") +

  geom_point(data = too_small, aes(x, y), size = 3, shape = 21, fill = "blue", alpha = .5) +

  geom_curve(data = too_small[-1, ], aes(x = x, y = y, xend = x2, yend = y2), curvature = 1, lty = "dotted") +

  theme(

    axis.ticks = element_blank(),

    axis.text = element_blank()

  ) +

  annotate("text", x = df[5, "x"], y = 1, label = "initial value", hjust = -0.1, vjust = .8)

# create too large of a learning rate

too_large <- df[round(which.min(df$y) * (1 + c(-.9, -.6, -.2, .3)), 0), ] %>%

  mutate(x2 = lag(x), y2 = lag(y))

# plot

ggplot(df, aes(x, y)) +

  geom_line(size = 1.5, alpha = .5) +

  theme_classic() +

  scale_y_continuous("Cost/Loss function of the variable", limits = c(0, 30)) +

  xlab("variable") +

  geom_segment(data = too_large[1, ], aes(x = x, y = y, xend = x, yend = -Inf), lty = "dashed") +

  geom_point(data = too_large, aes(x, y), size = 3, shape = 21, fill = "blue", alpha = .5) +

  geom_curve(data = too_large[-1, ], aes(x = x, y = y, xend = x2, yend = y2), curvature = 1, lty = "dotted") +

  theme(

    axis.ticks = element_blank(),

    axis.text = element_blank()

  ) +

  annotate("text", x = too_large[1, "x"], y = 1, label = "initial value", hjust = -0.1, vjust = .8)
```

## [Gradient Descent for Linear Regression]{.r-fit-text} {.smaller}

- **Objective**: We want to find $\hat{\beta}_0$ and $\hat{\beta}_1$ which minimizes
- $$MSE = \dfrac{e^2_1+e^2_2+\ldots+e^2_n}{n} = \dfrac{(\hat{y}_1 - y_1)^2 +  (\hat{y}_2 - y_2)^2 + \ldots + (\hat{y}_n - y_n)^2}{n}$$
- $$MSE = \dfrac{(\hat{\beta}_0 + \hat{\beta}_1 \ x_1 - y_1)^2 +  (\hat{\beta}_0 + \hat{\beta}_1 \ x_2 - y_2)^2 + \ldots + (\hat{\beta}_0 + \hat{\beta}_1 \ x_n - y_n)^2}{n}$$
- $$MSE = \displaystyle \dfrac{1}{n}\sum_{i=1}^{n} (\hat{\beta}_0 + \hat{\beta}_1 \ x_i - y_i)^2 = \displaystyle \dfrac{1}{n}\sum_{i=1}^{n} (\hat{y}_i - y_i)^2 = \displaystyle \dfrac{1}{n}\sum_{i=1}^{n} (e_i)^2$$

## [Gradient Descent for Linear Regression]{.r-fit-text} {.smaller}

- To compute gradient, need parital derivatives of $MSE$ with respect to $\hat{\beta}_0$ and $\hat{\beta}_1$.
- $$\text{gradient of MSE with respect to} \ \hat{\beta}_0 = \dfrac{2}{n} \displaystyle \sum_{i=1}^{n} (\hat{\beta}_0 + \hat{\beta}_1 \ x_i - y_i) = \dfrac{2}{n} \displaystyle \sum_{i=1}^{n} (\hat{y}_i - y_i)$$
- $$\text{gradient of MSE with respect to} \ \hat{\beta}_1 = \dfrac{2}{n} \displaystyle \sum_{i=1}^{n} x_i (\hat{\beta}_0 + \hat{\beta}_1 \ x_i - y_i) = \dfrac{2}{n} \displaystyle \sum_{i=1}^{n} x_i (\hat{y}_i - y_i)$$

## [Gradient Descent for Linear Regression]{.r-fit-text} {.smaller}

- Gradient descent update:
  + For $\hat{\beta}_0$:
    $$
    \begin{aligned}
    \hat{\beta}_0 \ \text{(new)} 
    &= \hat{\beta}_0 \ \text{(old)}- \bigg(\text{step size} \times \text{derivative w.r.t} \ \hat{\beta}_0\bigg)\\ 
    &= \hat{\beta}_0 \ \text{(old)}- \bigg(\text{step size} \times \dfrac{2}{n} \displaystyle \sum_{i=1}^{n} (\hat{y}_i - y_i)\bigg)
    \end{aligned}
    $$
  + For $\hat{\beta}_1$:
  $$
  \begin{aligned}
  \hat{\beta}_1  \ \text{(new)} &= \hat{\beta}_1  \ \text{(old)} - \bigg(\text{step size} \times \text{derivative w.r.t} \ \hat{\beta}_1 \bigg)\\
  &= \hat{\beta}_1  \ \text{(old)} - \bigg(\text{step size} \times \dfrac{2}{n} \displaystyle \sum_{i=1}^{n} x_i (\hat{y}_i - y_i)\bigg)
  \end{aligned}
  $$


## Linear Regression in R {.smaller}

::::{.columns}
:::{.column}
```{r}
outlets_model <- lm(profit ~ population, data = outlets)
outlets_model
```
:::

:::{.column}

This corresponds to the model:

$$
\begin{aligned}
\text{Profit}  &= -3.90 + 1.19\times\text{Population}\\
\hat{Y}_i &= -3.90 + 1.19X_i
\end{aligned}
$$
i.e. $\hat{\beta}_0 = -3.90$ and $\hat{\beta}_1 = 1.19$
:::
::::

## Linear Regression in R

```{r}

ggplot(data = outlets) +
  geom_point(mapping = aes(x = population, y = profit)) +    # create scatterplot
  geom_smooth(mapping = aes(x = population, y = profit), 
              method = "lm", se = FALSE)   # add the regression line
```

## Linear Regression in R: Prediction

```{r}
predict(outlets_model, newdata = data.frame(population = 17))
```

Note: New data must be a data frame with the same columns names as the training data
