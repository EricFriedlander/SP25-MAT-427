---
title: 'CMSC/LING/STAT 208: Machine Learning'
author: "Abhishek Chakraborty [Much of the content in these slides have been adapted from *An Introduction to Statistical Learning: with Applications in R*, James et al.]"
output: ioslides_presentation
#output: pdf_document
#output: html_document
# output: beamer_presentation


---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r, include=FALSE}
library(tidyverse)
# library(ggformula)
library(gridExtra)
# library(ISLR2)
# library(knitr)
# library(MASS)
# library(caret)
# library(pROC)
```

<style>
  .col2 {
    columns: 2 200px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 200px; /* chrome, safari */
    -moz-columns: 2 200px;    /* firefox */
  }
  .col3 {
    columns: 3 100px;
    -webkit-columns: 3 100px;
    -moz-columns: 3 100px;
  }
</style>


## Multiple Linear Regression

Response $Y$ and predictor variables $X_1, X_2, \ldots, X_p$. We assume

$$Y=f(\mathbf{X}) + \epsilon=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p + \epsilon$$

$\beta_j$ quantifies the association between the $j^{th}$ predictor and the response.



## Multiple Linear Regression: Estimating Parameters

We use training data to find $b_0, b_1, \ldots, b_p$ such that

$$\hat{y}=b_0 + b_1 \ x_1 + \ldots + b_p \ x_p$$
Observed response: $y_i$ for $i=1,\ldots,n$

Predicted response: $\hat{y}_i$ for $i=1, \ldots, n$

Residual: $e_i = \hat{y}_i - y_i$ for $i=1, \ldots, n$

Mean Squared Error (MSE): $MSE =\dfrac{e^2_1+e^2_2+\ldots+e^2_n}{n}$  also known as the **loss/cost function**

Problem: Find $b_0, b_1, \ldots, b_p$ which minimizes $MSE$
\
\
\

We can use **Gradient Descent** to minimize the $MSE$ with respect to $b_0, b_1, \ldots, b_p$.


## Multiple Linear Regression

**House Prices dataset**

* `size` is in square feet

* `num_bedrooms` is a count

* `price` is in $1,000

```{r}
house_prices <- readRDS("house_prices.rds")   # load dataset

head(house_prices, 6)   # first 6 observations
```


## Multiple Linear Regression

Some Exploratory Data Analysis (EDA)

```{r, fig.align='center', fig.width=8, fig.height=6}
library(GGally)

ggpairs(data = house_prices)   # correlation plot
```




## Multiple Linear Regression in R

**House Prices dataset**

```{r}

mlr_model <- lm(price ~ size + num_bedrooms, data = house_prices)   # fit the model

summary(mlr_model)   # produce result summaries of the model

```


## Multiple Linear Regression: Interpreting Parameters

**House Prices dataset**

<!-- * $\hat{\beta}_0=-2.242e+06$: With `Gr_Liv_Area` equaling 0 square feet, and `Year_Built` equaling 0, the predicted `Sale_Price` is approximately -2.242e+06 USD. The interpretation is not meaningful in this context. -->

* $b_1=0.1392$: With `num_bedrooms` remaining fixed, an additional 1 square foot of `size` leads to an increase in `price` by approximately $139.

<!-- * $\hat{\beta}_2=1.155e+03$: With `Gr_Liv_Area` remaining fixed, an additional 1 year on `Year_Built` leads to an increase in `Sale_Price` by approximately 1155 USD. -->


## Multiple Linear Regression: Prediction

**House Prices dataset**

Prediction of `price` when `size` is 2000 square feet for a house with 3 bedrooms.

```{r}

predict(mlr_model, newdata = data.frame(size = 2000, num_bedrooms = 3))   # obtain prediction

```



## Linear Regression: Comparing Models
<!-- ## <span style="color:blue">Your Turn!!!</span> -->

With the **House Prices** dataset, we create three models with `price` as the response:

<!-- ```{r} -->
<!-- advertising <- readRDS("Advertising.rds")   # load dataset -->
<!-- ``` -->

* **fit1**: a linear regression model with `num_bedrooms`  as the only predictor

* **fit2**: a linear regression model with `size` as the only predictor

* **mlr_model** (already created in the previous slides): a multiple regression model with `size` and `num_bedrooms` as predictors

<!-- For each model, note $p$ (the number of predictors), $R^2$, $\text{Adjusted} \ R^2$, $RSS$, and $RSE$. -->


## Linear Regression: Comparing Models

<!-- * Residual Sum of Squares (RSS) -->
* Mean Squared Error (MSE)

$$MSE = \displaystyle \sum_{i=1}^n e_i^2$$

* Residual Standard Error (RSE)

$$RSE=\sqrt{\dfrac{MSE}{n-p-1}}$$


<!-- * $R^2$ statistic -->

<!-- $$R^2=\dfrac{TSS-RSS}{TSS} = 1 - \dfrac{RSS}{TSS}$$ -->


<!-- * Adjusted $R^2$ statistic -->

<!-- $$\text{Adjusted} \ R^2 = 1 - \dfrac{RSS/(n-p-1)}{TSS/(n-1)}$$ -->


<!-- ## <span style="color:blue">Your Turn!!!</span> -->

<!-- With the **Advertising** dataset, create three models with **sales** as response: -->

<!-- ```{r} -->
<!-- advertising <- readRDS("Advertising.rds")   # load dataset -->
<!-- ``` -->

<!-- * `fit1`: a linear regression model with **TV** as the only predictor -->

<!-- * `fit2`: a linear regression model with **TV** and **radio** as predictors -->

<!-- * `fit3`: a linear regression model with **TV**, **radio**, and **newspaper** as predictors -->

<!-- For each model, note $p$ (the number of predictors), $R^2$, $\text{Adjusted} \ R^2$, $RSS$, and $RSE$. -->


<!-- ## Linear Regression: Assessing Accuracy of Model -->

<!-- ```{r} -->
<!-- round(cor(advertising), 3)   # obtain correlation matrix -->
<!-- ``` -->


<!-- ## <span style="color:blue">Question!!!</span> -->

<!-- As we add variables to the linear regression model, (Select all that apply) -->

<!-- 1. the RSE always decreases. -->

<!-- 2. the RSS always decreases. -->

<!-- 3. the $R^2$ always increases. -->

<!-- 4. the $\text{Adjusted} \ R^2$ always increases. -->

<!-- 5. the number of parameters always increases. -->


<!-- ## <span style="color:blue">Question!!!</span> -->

<!-- Consider the population model $Y = \beta_0 + \beta_1 X + \epsilon$. The estimated model is $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$. Reflecting on the concepts from week 1, fill in the blanks below. -->

<!-- 1. If $\beta_0$ and $\beta_1$ were known ("truth" known), the discrepancy between response $Y$ and $\beta_0 + \beta_1 X$ is related to the $\underline{\hspace{5cm}}$ (irreducible/reducible) error. -->

<!-- 2. The inaccuracy of $\hat{\beta}_0$ and $\hat{\beta}_1$ as estimates of $\beta_0$ and $\beta_1$ is related to the $\underline{\hspace{5cm}}$ (irreducible/reducible) error. -->


<!-- ## <span style="color:blue">Question!!!</span> -->

<!-- The least squares approach -->

<!-- * minimizes the sum of squared predictor values. -->

<!-- * minimizes the sum of squared response values. -->

<!-- * minimizes the sum of squared residuals. -->

<!-- * maximizes the sum of squared residuals. -->



## Regression: Conditional Averaging

**Restaurant Outlets Profit dataset**

```{r, fig.align='center', fig.height=4, fig.width=7, echo=FALSE}
outlets <- readRDS("outlets.rds")   # load dataset

ggplot(data = outlets, aes(x = population, y = profit)) +
  geom_point() +   # create scatterplot
  geom_smooth(method = "lm", se = FALSE)   # add the SLR line
```


What is a good value of $\hat{f}(x)$ (expected profit), say at $x=6$?

A possible choice is the **average of the observed responses** at $x=6$. But we may not observe responses for certain $x$ values.


<!-- ## Regression: Conditional Averaging -->

<!-- What is a good value of $\hat{f}(x)$, say at $x=6$? A possible value is the **average of the observed responses** at $x=6$. -->

<!-- $$\hat{f}(x)=E(Y|x=1008)$$ -->

<!-- $E(Y|x=1008)$ means **expected value**, or, the **average of the observed responses** at $x=1008$. -->

<!-- But we may not observe responses for certain $x$ values. -->


## K-Nearest Neighbors Regression

* Non-parametric approach

* Given a value for $K$ and a test data point $x_0$,

$$\hat{f}(x_0)=\dfrac{1}{K} \sum_{x_i \in \mathcal{N}_0} y_i=\text{Average} \ \left(y_i \ \text{for all} \ i:\ x_i \in \mathcal{N}_0\right) $$

where $\mathcal{N}_0$ is known as the **neighborhood** of $x_0$.


* The method is based on the concept of closeness of $x_i$'s from $x_0$ for inclusion in the neighborhood $\mathcal{N}_0$. Usually, the **Euclidean distance** is used as a measure of closeness. The Euclidean distance between two $p$-dimensional vectors $\mathbf{a}=(a_1, a_2, \ldots, a_p)$ and $\mathbf{b}=(b_1, b_2, \ldots, b_p)$ is

$$||\mathbf{a}-\mathbf{b}||_2 = \sqrt{(a_1-b_1)^2 + (a_2-b_2)^2 + \ldots + (a_p-b_p)^2}$$


## K-Nearest Neighbors Regression (single predictor): Fit

**Restaurant Outlets Profit dataset**

```{r, message=FALSE}
library(caret)   # load the caret package
```

* **1-NN regression**

```{r}
knnfit1 <- knnreg(profit ~ population, data = outlets, k = 1)   # 1-nn regression
```

```{r}
predict(knnfit1, newdata = data.frame(population = 6))  # 1-nn prediction
```


* **5-NN regression**

```{r}
knnfit5 <- knnreg(profit ~ population, data = outlets, k = 5)   # 5-nn regression
```

```{r}
predict(knnfit5, newdata = data.frame(population = 6))  # 5-nn prediction
```

<!-- ## K-Nearest Neighbors Regression (single predictor): Prediction -->

<!-- **Ames Housing dataset** -->

<!-- ```{r} -->
<!-- nearest_neighbors <- ames %>% -->
<!--   select(Sale_Price, Gr_Liv_Area) %>% -->
<!--   mutate(distance = sqrt((1008-Gr_Liv_Area)^2)) %>%   # calculate distance -->
<!--   arrange(distance)   # sort by increasing distance -->
<!-- ``` -->

<!-- ```{r} -->
<!-- predict(knnfit1, newdata = data.frame(population = 1008))  # 1-nn prediction -->
<!-- ``` -->

<!-- ```{r} -->
<!-- predict(knnfit5, newdata = data.frame(population = 1008))  # 5-nn prediction -->
<!-- ``` -->


## Regression Methods: Comparison

**Restaurant Outlets Profit dataset**

```{r, echo=FALSE, fig.align='center', fig.height=5, fig.width=8, fig.cap="dashed cyan: 1-nn fit, dotted red: 5-nn fit, blue: linear regression fit"}
slrfit <- lm(profit ~ population, data = outlets)   # fit the SLR model

pop_seq <- seq(min(outlets$population, na.rm = TRUE), max(outlets$population, na.rm = TRUE), 0.01)

# obtain predictions for all training data points
# knn_1 <- predict(knnfit1, newdata = data.frame(population = min(outlets$population, na.rm = TRUE):max(outlets$population, na.rm = TRUE)))
knn_1 <- predict(knnfit1, newdata = data.frame(population = pop_seq))
# knn_5 <- predict(knnfit5, newdata = data.frame(population = min(outlets$population, na.rm = TRUE):max(outlets$population, na.rm = TRUE)))
knn_5 <- predict(knnfit5, newdata = data.frame(population = pop_seq))
# p <-  predict(slrfit, newdata = data.frame(population = min(outlets$population, na.rm = TRUE):max(outlets$population, na.rm = TRUE)))
p <-  predict(slrfit, newdata = data.frame(population = pop_seq))

# column bind original data with predicted values
# ames1 <- outlets %>% select(profit, population) %>% filter(!is.na(population))
# predictions <- data.frame(population = min(outlets$population, na.rm = TRUE):max(outlets$population, na.rm = TRUE),
                          # linear = p, knn_1, knn_5)
predictions <- data.frame(population = pop_seq,
                          linear = p, knn_1, knn_5)

# plot the three models
ggplot(data = outlets, aes(x = population, y = profit)) +
  geom_point() +
  geom_line(data = predictions, aes(x = population, y = knn_1), color = "cyan", linetype = "dashed", linewidth = 1) +   # 1-nn regression
  geom_line(data = predictions, aes(x = population, y = knn_5), color = "red", linetype = "dotted", linewidth = 1) +   # 5-nn regression
  geom_line(data = predictions, aes(x = population, y = linear), color = "blue", linewidth = 1) +
  theme_bw()

```


<!-- ## <span style="color:blue">Your Turn!!!</span> -->

<!-- For the **Advertising** dataset, create a 10-nearest neighbors fit **knnfit10** with **sales** as response and **TV** as predictor. Obtain the predicted **sales** for $\text{TV} = 225,000$. -->


## <span style="color:blue">Question!!!</span>

As $k$ in KNN regression increases,

* the flexibility of the fit $\underline{\hspace{5cm}}$ (increases/decreases)

* the bias of the fit $\underline{\hspace{5cm}}$ (increases/decreases)

* the variance of the fit $\underline{\hspace{5cm}}$ (increases/decreases)


## K-Nearest Neighbors Regression (multiple predictors)

It is important to **scale (subtract mean and divide by standard deviation)** the predictors when considering KNN regression so that the Euclidean distance is not dominated by a few of them with large values.

**House Prices dataset**

```{r}

# scale predictors
house_prices_scaled <- data.frame(size_scaled = scale(house_prices$size),
                                  num_bedrooms_scaled = scale(house_prices$num_bedrooms),
                                  price = house_prices$price)

head(house_prices_scaled)   # first six observations

```


## K-Nearest Neighbors Regression (multiple predictors)

**House Prices dataset**

```{r, message=FALSE}
library(caret)   # load library
```

```{r}

knnfit10 <- knnreg(price ~ size_scaled + num_bedrooms_scaled, data = house_prices_scaled, k = 10)   # 10-nn regression

```


It is also important to apply scaling to test data points before prediction. Suppose, you want predictions for `size` = 2000 square feet, and `num_bedrooms` = 3, then

```{r}
# obtain 10-nn prediction

predict(knnfit10, newdata = data.frame(size_scaled = (2000 - mean(house_prices$size, na.rm = TRUE))/sd(house_prices$size, na.rm = TRUE),
                                     num_bedrooms_scaled = (3 - mean(house_prices$num_bedrooms))/sd(house_prices$num_bedrooms)))
```


## Linear Regression vs K-Nearest Neighbors


* Linear regression is a parametric approach (with restrictive assumptions), KNN is non-parametric.

* Linear regression works for regression problems ($Y$ numerical), KNN can be used for both regression and classification ($Y$ qualitative).

* Linear regression is interpretable, KNN is not.

* Linear regression can accommodate qualitative predictors and can be extended to include interaction terms as well. Using Euclidean distance with KNN does not allow for qualitative predictors.

* In terms of prediction, KNN can be pretty good for small $p$, that is, $p \le 4$ and large $n$. Performance of KNN deteriorates as $p$ increases - curse of dimensionality.



## Classification Problems

* Response $Y$ is qualitative (categorical).

* The objective is to build a classifier $\hat{Y}=\hat{C}(\mathbf{X})$ that assigns a class label to a future unlabeled (unseen) observation and understand the relationship between the predictors and response.

* There can be two types of predictions based on the research problem.

  + Class probabilities
  
  + Class labels

<!-- Often we are more interested in estimating the probabilities than actual class labels. -->


## Classification Problems: Example

<!-- ## Logistic Regression -->

<!-- * Supervised learning -->

<!-- * Classification (even though the term contains "Regression") -->

<!-- * Parametric approach -->

**Default dataset**

```{r, message=FALSE}
library(ISLR2)   # load library
data("Default")   # load dataset
```

```{r}
head(Default)   # print first six observations
```

```{r,message=FALSE}
table(Default$default)   # class frequencies
```


**We will consider `default` as the response variable.**


## Classification Problems: Example

For some algorithms, we might need to convert the categorical response to numeric (0/1) values.

**Default dataset**

```{r,message=FALSE}
Default$default_id <- ifelse(Default$default == "Yes", 1, 0)   # create 0/1 variable

head(Default, 10)   # print first ten observations
```




## K-Nearest Neighbors Classifier

Given a value for $K$ and a test data point $x_0$,
$$P(Y=j | X=x_0)=\dfrac{1}{K} \sum_{x_i \in \mathcal{N}_0} I(y_i = j)$$

where $\mathcal{N}_0$ is known as the **neighborhood** of $x_0$.


For classification problems, the predictions are obtained in terms of **majority vote** (unlike in regression where predictions are obtained by averaging).


## K-Nearest Neighbors Classifier: Build Model 

**Default dataset**

response ($Y$): `default` and predictor ($X$): `balance`

```{r}
library(caret)   # load package 'caret'

knnfit <- knn3(default ~ balance, data = Default, k = 10)   # fit 10-nn model
```


## K-Nearest Neighbors Classifier: Predictions 

**Default dataset**

* One can directly obtain the class label predictions as below.

```{r}
knn_class_preds_1 <- predict(knnfit, newdata = Default, type = "class")   # obtain default class label predictions
```


* Otherwise, one can first obtain predictions in terms of probabilities and then convert them into class label predictions based on a threshold.

```{r}
knn_prob_preds <- predict(knnfit, newdata = Default, type = "prob")   # obtain predictions as probabilities
```


```{r}
threshold <- 0.5   # set threshold

knn_class_preds_2 <- factor(ifelse(knn_prob_preds[,2] > threshold, "Yes", "No"))   # obtain predictions as class labels
```


## K-Nearest Neighbors Classifier: Performance  {.smaller}

**Default dataset**

```{r}
# create confusion matrix

# use the following code only when all predictions are from the same class
# levels(knn_class_preds_1) = c("No", "Yes") 

confusionMatrix(data = knn_class_preds_1, reference = Default$default, positive = "Yes")   
```


