---
title: 'CMSC/LING/STAT 208: Machine Learning'
author: "Abhishek Chakraborty [Much of the content in these slides have been adapted from *ISLR2* by James et al. and *HOMLR* by Boehmke & Greenwell]"
output: ioslides_presentation
#output: pdf_document
#output: html_document
# output: beamer_presentation
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r, include=FALSE}
library(tidyverse)
# library(ggformula)
library(gridExtra)
# library(ISLR2)
# library(knitr)
# library(MASS)
library(caret)
# library(pROC)
library(recipes)
library(vip)
```

<style>
  .col2 {
    columns: 2 200px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 200px; /* chrome, safari */
    -moz-columns: 2 200px;    /* firefox */
  }
  .col3 {
    columns: 3 100px;
    -webkit-columns: 3 100px;
    -moz-columns: 3 100px;
  }
</style>


## Remedies for Class Imbalance

For binary classification problems, in some contexts, the class of interest (`Positive` class) might have a very low frequency. We will see how to approach the ML process when such cases arise.

We will look at three possible remedies.

* Model tuning

* Alternative cutoffs

* Subsampling

We will work with the `attrition.rds` dataset.



## Linear Model Selection and Regularization

The standard linear model,

```{r , echo=FALSE,  fig.align='center', out.width = '60%'}
knitr::include_graphics("EFT/e6.1.png")
```

* **Benefits**: Simple, interpretable, often shows good predictive performance.

* **Limitations**:

  - **Prediction Accuracy**: If $n$ is not big (also when $p>n$), variance can be high resulting in overfitting and poor predictive performance.

  - **Model Interpretability**: Irrelevant features lead to unnecessary model complexity.


## Alternatives to Least Squares

Extensions/Modifications/Improvements to Least Squares:

* **Subset Selection**: Identify a subset of $p$ predictors and then fit a linear model using least squares.

* **Shrinkage/Regularization**: Fit a model using $p$ predictors, but shrink some of the estimated coefficients towards zero. Reduces variance and can also perform variable selection.

* **Dimension Reduction**: Project $p$ predictors onto a $M$-dimensional subspace, where $M<p$. Achieved by computing $M$ different linear combinations or projections of the $p$ predictors. Fit a linear model using these $M$ predictors by least squares.


## Shrinkage/Regularization Methods

Fit a model containing all $p$ predictors using a technique that **shrinks** the coefficient estimates towards zero.

* Ridge Regression

* Lasso

**Shrinking the coefficient estimates significantly reduces their variance.**


## The Lasso {.smaller}

Acronym for **Least Absolute Shrinkage and Selection Operator**.

* **Standard Linear Model**

Given a training dataset, for $i=1,\ldots,n$

$$\hat{y}_i=b_0+b_1 x_{i1}+ \ldots + b_p x_{ip}$$

<!-- $$y_i=b_0+\beta_1 x_{i1}+ \ldots + \beta_p x_{ip} + \epsilon_i = \beta_0 + \displaystyle \sum_{j=1}^p \beta_j x_{ij} + \epsilon_i$$ -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '30%'} -->
<!-- knitr::include_graphics("EFT/e6.45.png") -->
<!-- ``` -->

$$\text{SSE} = \displaystyle \sum_{i=1}^{n} \bigg(y_i - \hat{y}_i\bigg)^2 = \displaystyle \sum_{i=1}^{n} \bigg(y_i - (b_0+b_1 x_{i1}+ \ldots + b_p x_{ip})\bigg)^2 $$

* **Lasso**

<!-- Lasso is a relatively recent alternative to ridge regression that overcomes the disadvantage. -->

<!-- ```{r , echo=FALSE, fig.align='center', out.width = '60%'} -->
<!-- knitr::include_graphics("EFT/e6.7.png") -->
<!-- ``` -->

$$\displaystyle \text{SSE} + \lambda \sum_{j=1}^{p} |b_j|$$

* $\lambda \displaystyle \sum_{j=1}^{p} |b_j|$: **Shrinkage Penalty**

* $\lambda \ge 0$: **Tuning/Regularization Parameter**


<!-- ```{r , echo=FALSE, out.width = '25%'} -->
<!-- knitr::include_graphics("EFT/l1.png") -->
<!-- ``` -->


## The Lasso

<!-- * Like ridge regression, the lasso shrinks the coefficient estimates towards zero. -->

<!-- * However, in the case of the lasso, the $l_1$ penalty forces some of the coefficient estimates to be -->
<!-- exactly equal to zero when the tuning parameter $\lambda$ is sufficiently large. -->

* This method not only shrinks the coefficient estimates towards zero, but also makes some of the coefficient estimates exactly equal to zero (when the tuning parameter $\lambda$ is sufficiently large).

* Hence, the lasso performs **variable selection**.

<!-- much like best subset selection, -->

* We say that the lasso yields **sparse models**, that is, models that involve only a subset of the variables.



## The Lasso: Scaling of Predictors

* Standard least squares (regression) coefficient estimates are **scale equivariant**: multiplying $X_j$ by a constant $c$ simply leads to a scaling of the least squares coefficient estimates by a factor of $1/c$. In other words, regardless of how the $j^{th}$ predictor is scaled, $X_j \hat{\beta}_j$ will remain the same.

* In contrast, the lasso coefficient estimates can change substantially when multiplying a given predictor by a constant. Apply lasso after **standardizing** the predictors.

```{r , echo=FALSE, fig.align='center', out.width = '50%'}
knitr::include_graphics("EFT/e6.6.png")
```



## The Lasso: Implementation

**Ames Housing Dataset**

```{r}
ames <- readRDS("AmesHousing.rds")   # load dataset
```

```{r}
# reorder levels of 'Overall_Qual'
ames$Overall_Qual <- factor(ames$Overall_Qual, levels = c("Very_Poor", "Poor", "Fair", "Below_Average",
                                                          "Average", "Above_Average", "Good", "Very_Good",
                                                          "Excellent", "Very_Excellent"))
```

```{r}
# split data

set.seed(050724)   # set seed

train_index <- createDataPartition(y = ames$Sale_Price, p = 0.7, list = FALSE)   # consider 70-30 split

ames_train <- ames[train_index,]   # training data

ames_test <- ames[-train_index,]   # test data
```


## The Lasso: Implementation

**Ames Housing Dataset**

```{r}
# create recipe and blueprint, prepare and apply blueprint

set.seed(050724)   # set seed

ames_recipe <- recipe(Sale_Price ~ ., data = ames_train)   # set up recipe

blueprint <- ames_recipe %>%
  step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %>%   # filter out zv/nzv predictors
  step_impute_mean(Gr_Liv_Area) %>%                                    # impute missing entries
  step_integer(Overall_Qual) %>%                                       # numeric conversion of levels of the predictors
  step_center(all_numeric(), -all_outcomes()) %>%                      # center (subtract mean) all numeric predictors
  step_scale(all_numeric(), -all_outcomes()) %>%                       # scale (divide by standard deviation) all numeric predictors
  step_other(Neighborhood, threshold = 0.01, other = "other") %>%      # lumping required predictors
  step_dummy(all_nominal(), one_hot = FALSE)                           # one-hot/dummy encode nominal categorical predictors


prepare <- prep(blueprint, data = ames_train)    # estimate feature engineering parameters based on training data


baked_train <- bake(prepare, new_data = ames_train)   # apply blueprint to training data

baked_test <- bake(prepare, new_data = ames_test)    # apply blueprint to test data
```


## The Lasso: Implementation

**Ames Housing Dataset**

Implement CV to tune the hyperparameter $\lambda$.

```{r}
set.seed(050724)   # set seed

cv_specs <- trainControl(method = "repeatedcv", number = 5, repeats = 5)   # CV specifications

lambda_grid <- 10^seq(-3, 4, length = 100)   # grid of lambda values to search over

library(glmnet)  # for LASSO

lasso_cv <- train(blueprint,
                   data = ames_train,
                   method = "glmnet",   # for lasso
                   trControl = cv_specs,
                   tuneGrid = expand.grid(alpha = 1, lambda = lambda_grid),   # alpha = 1 implements lasso
                   metric = "RMSE")

# results from the CV procedure

lasso_cv$bestTune$lambda    # optimal lambda

min(lasso_cv$results$RMSE)   # RMSE for optimal lambda

```


## The Lasso: Implementation

**Ames Housing Dataset**

Results from the CV procedure.

```{r, fig.align='center', fig.height=6, fig.width=8}
ggplot(lasso_cv)   # lambda vs. RMSE plot
```


## The Lasso: Implementation

**Ames Housing Dataset**

We will now build the optimal lasso model on the modified training data using the optimal $\lambda$.

```{r}
# create datasets required for 'glmnet' function

X_train <- model.matrix(Sale_Price ~ ., data = baked_train)[, -1]   # training features without intercept

Y_train <- baked_train$Sale_Price    # training response

X_test <- model.matrix(Sale_Price ~ ., data = baked_test)[, -1]   # test features without intercept
```

```{r}
# build optimal lasso model

final_model <- glmnet(x = X_train,
                      y = Y_train,
                      alpha = 1,                           # alpha = 1 builds lasso model
                      lambda = lasso_cv$bestTune$lambda,   # using optimal lambda from CV
                      standardize = FALSE)                 # already standardized during data preprocessing
```


```{r}
# obtain predictions and test set RMSE

final_model_preds <- predict(final_model, newx = X_test)    # obtain predictions

sqrt(mean((final_model_preds - baked_test$Sale_Price)^2))   # calculate test set RMSE
```


## The Lasso: Implementation {.smaller}

**Ames Housing Dataset**

The coefficients for the optimal lasso model can be obtained from

```{r}
coef(final_model)    # estimated coefficients from final lasso model
```


## The Lasso: Implementation {.smaller}

**Ames Housing Dataset**

```{r, fig.align='center', fig.height=6, fig.width=8}
# variable importance

vip(object = lasso_cv, num_features = 20, method = "model")
```



<!-- ## <span style="color:blue">Your Turn!!!</span> {.smaller} -->

<!-- You will work with the **Hitters.rds** dataset. Please download the dataset from Canvas, upload it to Posit Cloud, and load it using the following code. -->

<!-- ```{r} -->
<!-- Hitters <- readRDS("Hitters.rds")   # load dataset -->
<!-- ``` -->


<!-- The dataset contains baseball statistics from the 1986 and 1987 seasons. The task is to predict `Salary` using the rest of the variables in the dataset. Compare the performance (in terms of **RMSE**) of the following two models: -->

<!-- * A linear regression model; -->

<!-- * A LASSO model chosen by CV. Consider the grid of possible $\lambda$ values as `lambda_grid <- 10^seq(-2, 2, length = 100)`. -->


<!-- **Perform the following tasks.** -->

<!-- * Investigate the dataset and complete any necessary tasks. -->

<!-- * Split the data into training and test sets (80-20). -->

<!-- * Perform required data preprocessing and create the blueprint. If using `step_dummy()`, set `one_hot = FALSE`. Prepare the blueprint on the training data. Obtain the modified training and test datasets. -->

<!-- * Implement 5-fold CV repeated 5 times for each of the models above. -->

<!-- * Report the optimal CV RMSE of each model. Report the optimal value of $\lambda$ for the LASSO model. Which model performs better in this situation? -->

<!-- * Using the optimal model, obtain predictions on the test set. Calculate and report the test set RMSE. -->

<!-- * Using the optimal model, obtain variable importance measures for the features. -->



## Tree-Based Methods

* Involves **stratifying** or **segmenting** the predictor space into a number of simple regions.

* The set of splitting rules used to segment the predictor space can be summarized in a tree, thus, the name **decision tree** methods.

* Can be used for both classification and regression.

* Tree-based methods are simple and useful for interpretation, however, not the best in terms of prediction accuracy.

* Methods such as **bagging**, **random forests**, and **boosting** grow multiple trees and then combine their results.

<!-- ## Regression Trees -->

<!-- **Hitters dataset** -->

<!-- ```{r,echo=FALSE} -->
<!-- data("Hitters") -->
<!-- Hitters=na.omit(Hitters) -->
<!-- head(Hitters) -->
<!-- ``` -->

<!-- ## Regression Trees -->

<!-- **Hitters dataset** -->

<!-- log(Salary) is color-coded from low (blue, green) to high (yellow, red). -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '60%'} -->
<!-- knitr::include_graphics("EFT/SL_C8_1.png") -->
<!-- ``` -->

<!-- ## Regression Trees -->

<!-- **Hitters dataset** -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '60%'} -->
<!-- knitr::include_graphics("EFT/8.1.png") -->
<!-- ``` -->

<!-- ## Regression Trees -->

<!-- **Hitters dataset** -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->
<!-- knitr::include_graphics("EFT/8.2.png") -->
<!-- ``` -->



## Terminology for Trees

* Every split is considered to be a **node**.

* We refer to the first node at the top of the tree as the **root node** (this node contains all of the training data). 

* The final nodes at the bottom of the tree are called the **terminal nodes** or **leaves**.

<!-- The regions $R_1$, $R_2$, and $R_3$ are known as **terminal nodes** or **leaves**. -->

* Decision trees are typically drawn **upside down**, in the sense that the leaves are at the bottom of the tree.

* The points along the tree where the predictor space is split are referred to as **internal nodes**, that is, every node in between the **root node** and **terminal nodes** is referred to as an **internal node**.

* The segments of the trees that connect the nodes are known as **branches**.


## Terminology for Trees

```{r , echo=FALSE,  fig.align='center', fig.cap="Adapted from HMLR, Boehmke & Greenwell", out.width = '100%'}
knitr::include_graphics("EFT/tree2.jpg")
```


<!-- ## Interpretation of Trees -->

<!-- **Hitters dataset** -->

<!-- * **Years** is the most important factor in determining **Salary**, and players with less experience earn lower salaries than more experienced players. -->

<!-- * Given that a player is less experienced, the number of **Hits** that he made in the previous year seems to play little role in his **Salary**. -->

<!-- * But among players who have been in the major leagues for five or more years, the number of **Hits** made in the previous year does affect **Salary**, and players who made more **Hits** last year tend to have higher salaries. -->

<!-- * Surely an over-simplification, but compared to a regression model, it is easy to display, interpret and explain. -->


## Building a Tree

* First select the predictor $X_j$ and the cutpoint $s$ such that splitting the predictor space into the regions
$\{X|X_j < s\}$ and $\{X|X_j \ge s\}$ leads to the greatest possible reduction in $SSE$.

For any $j$ and $s$, define

$$R_1 = \{X|X_j < s\} \ \ \text{and} \ \ R_2 = \{X|X_j > s\}$$

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '67%'} -->
<!-- knitr::include_graphics("EFT/e8.2.png") -->
<!-- ``` -->

Find $j$ and $s$ that minimize

$$SSE = \displaystyle \sum_{i \in R_1}\left(y_i - \hat{y}_{R_1}\right)^2 + \sum_{i \in R_2}\left(y_i - \hat{y}_{R_2}\right)^2$$

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '67%'} -->
<!-- knitr::include_graphics("EFT/e8.3.png") -->
<!-- ``` -->

* Next, repeat the process, look for the best predictor and best cutpoint in order to split the data further.
However, this time, instead of splitting the entire predictor space, split one of the two previously identified regions.

* The process continues until a stopping criterion is reached; say, we may continue until no region contains more than five observations.





## Prediction

<!-- Building a tree involves two steps. -->

<!-- The steps involved are: -->

<!-- 1. Dividing the predictor space, that is, the set of possible values for $X_1, X_2, \ldots, X_p$ into $J$ distinct and non-overlapping regions, $R_1, R_2, \ldots, R_J$. -->

For every observation that falls into the region $R_j$, make the same prediction, which is 

  * the mean response of the training set observations in $R_j$ (for regression problems), 
  
  * majority vote response of the training set observations in $R_j$ (for classification problems).


## Building a Tree and Prediction

```{r , echo=FALSE,  fig.align='center', fig.cap="Adapted from HMLR, Boehmke & Greenwell", out.width = '100%'}
knitr::include_graphics("EFT/tree1.jpg")
```


## Building a Tree and Prediction

```{r , echo=FALSE,  fig.align='center', fig.cap="Adapted from HMLR, Boehmke & Greenwell", out.width = '100%'}
knitr::include_graphics("EFT/tree2.jpg")
```


## Building a Tree and Prediction

**Default Dataset**

```{r, fig.align='center', fig.height=6, fig.width=8, echo=FALSE}
library(ISLR2)

data("Default")

ggplot(data = Default) +
  geom_point(aes(x = balance, y = income, color = default), shape = c(3), alpha = 10)
```


## Building a Tree and Prediction

**Default Dataset**

```{r, fig.align='center', fig.height=6, fig.width=8, echo=FALSE}
library(rpart)
library(rpart.plot)
t <- rpart(default ~ balance + income, data = Default, method = "class")
rpart.plot(t)
```


## Building a Tree and Prediction

```{r , echo=FALSE,  fig.align='center', fig.cap="Adapted from ISLR, James et al.", out.width = '60%'}
knitr::include_graphics("EFT/8.3.png")
```


<!-- ## Building a Tree -->

<!-- * In theory, regions could have any shape. However, we -->
<!-- choose to divide the predictor space into **high-dimensional rectangles**, or **boxes**, for simplicity and for ease of interpretation of the resulting predictive model. -->

<!-- * **Objective**: Find boxes $R_1, R_2, \ldots R_J$ that minimize the $RSS$, -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '40%'} -->
<!-- knitr::include_graphics("EFT/e8.1.png") -->
<!-- ``` -->

<!-- where $\hat{y}_{R_j}$ is the mean response for the training observations within the $j^{th}$ box. -->


## Building a Tree

* It is computationally infeasible to consider every possible partition of the feature space into $J$ boxes.

* For this reason, we take a **top-down, greedy** approach known as **recursive binary splitting**.
    + **top-down** because it begins at the top of the tree and then successively splits the predictor space.
    + **greedy** because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.



<!-- ## Building a Tree -->

<!-- ```{r , echo=FALSE,  fig.align='center', fig.cap="Adapted from HMLR, Boehmke & Greenwell", out.width = '100%'} -->
<!-- knitr::include_graphics("tree2.jpg") -->
<!-- ``` -->




## Tree Pruning

The process described above may **overfit** the data.

```{r , echo=FALSE,  fig.align='center', out.width = '70%'}
knitr::include_graphics("EFT/pruning.png")
```



<!-- ## Tree Pruning -->

<!-- * The process described above may **overfit** the data. -->

<!-- * A smaller tree with fewer splits (that is, fewer regions $R_1,\ldots,R_J$) might lead to lower variance and better interpretation at the cost of a little bias. -->

<!-- * One possible alternative is to grow the tree only so long as the decrease in the $RSS$ due to each split exceeds some (high) threshold. -->

<!-- * This strategy will result in smaller trees, but is too short-sighted: a seemingly worthless split early on in the tree might be followed by a very good split, that is, a split that leads to a large reduction in $RSS$ later on. -->



## Tree Pruning

<!-- * A better strategy is **pruning**. -->

* Grow a very large tree, and then **prune** it back to obtain a **subtree**.

<!-- $T_0$ -->

* The technique uses is known as **cost complexity pruning** (also known as **weakest link pruning**).

* Consider a sequence of trees indexed by $\alpha$. For each $\alpha$, consider the tree that minimizes

$$SSE + \alpha \ |T|$$


<!-- $T \subset T_0$ -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->
<!-- knitr::include_graphics("EFT/e8.4.png") -->
<!-- ``` -->

* Choose optimal $\alpha$ by CV.


<!-- ## Building an Optimal Tree -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '90%'} -->
<!-- knitr::include_graphics("EFT/algo8.1.png") -->
<!-- ``` -->

<!-- ## Building an Optimal Tree -->

<!-- **Hitters dataset** -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->
<!-- knitr::include_graphics("EFT/8.4.png") -->
<!-- ``` -->

<!-- ## Building an Optimal Tree -->

<!-- **Hitters dataset** -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '90%'} -->
<!-- knitr::include_graphics("EFT/8.5.png") -->
<!-- ``` -->

## Regression Tree: Implementation

**Ames Housing Dataset**

```{r}
ames <- readRDS("AmesHousing.rds")   # load dataset
```

```{r}
# reorder levels of 'Overall_Qual'
ames$Overall_Qual <- factor(ames$Overall_Qual, levels = c("Very_Poor", "Poor", "Fair", "Below_Average", 
                                                          "Average", "Above_Average", "Good", "Very_Good", 
                                                          "Excellent", "Very_Excellent"))
```

```{r}
# split data

set.seed(050924)   # set seed

train_index <- createDataPartition(y = ames$Sale_Price, p = 0.7, list = FALSE)   # consider 70-30 split

ames_train <- ames[train_index,]   # training data

ames_test <- ames[-train_index,]   # test data
```


## Regression Tree: Implementation

**Ames Housing Dataset**

```{r}
# create recipe and blueprint, prepare and apply blueprint

set.seed(050924)   # set seed

ames_recipe <- recipe(Sale_Price ~ ., data = ames_train)   # set up recipe

blueprint <- ames_recipe %>%
  step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %>%   # filter out zv/nzv predictors
  step_impute_mean(Gr_Liv_Area) %>%                                    # impute missing entries
  step_integer(Overall_Qual) %>%                                       # numeric conversion of levels of the predictors
  step_center(all_numeric(), -all_outcomes()) %>%                      # center (subtract mean) all numeric predictors
  step_scale(all_numeric(), -all_outcomes()) %>%                       # scale (divide by standard deviation) all numeric predictors
  step_other(Neighborhood, threshold = 0.01, other = "other") %>%      # lumping required predictors
  step_dummy(all_nominal(), one_hot = FALSE)                           # one-hot/dummy encode nominal categorical predictors


prepare <- prep(blueprint, data = ames_train)    # estimate feature engineering parameters based on training data


baked_train <- bake(prepare, new_data = ames_train)   # apply the blueprint to training data

baked_test <- bake(prepare, new_data = ames_test)    # apply the blueprint to test data
```


## Regression Tree: Implementation

**Ames Housing Dataset**

Implement CV to tune the hyperparameter.

```{r}
set.seed(050924)   # set seed

cv_specs <- trainControl(method = "repeatedcv", number = 5, repeats = 5)   # CV specifications


library(rpart)   # for trees

tree_cv <- train(blueprint,
                 data = ames_train,
                 method = "rpart",  
                 trControl = cv_specs,
                 tuneLength = 20,                   # considers a grid of 20 possible tuning parameter values
                 metric = "RMSE")
```


```{r}
# results from the CV procedure

tree_cv$bestTune    # optimal hyperparameter

min(tree_cv$results$RMSE)   # optimal CV RMSE
```


## Regression Tree: Implementation {.smaller}

**Ames Housing Dataset**

Results from the CV procedure.

```{r, fig.align='center', fig.height=6, fig.width=8}
ggplot(tree_cv)   
```


## Regression Tree: Implementation

**Ames Housing Dataset**

```{r}
# build final model

final_model <- rpart(formula = Sale_Price ~ .,
                     data = baked_train,
                     cp = tree_cv$bestTune$cp,
                     xval = 0,                 # no further CV
                     method = "anova")         # for regression
```


```{r}
# obtain predictions and test set RMSE

final_model_preds <- predict(object = final_model, newdata = baked_test, type = "vector")    # obtain test set predictions

sqrt(mean((final_model_preds - baked_test$Sale_Price)^2))   # calculate test set RMSE
```


## Regression Tree: Implementation 

**Ames Housing Dataset**

```{r, fig.align='center', fig.height=6, fig.width=8}
library(rpart.plot)
rpart.plot(final_model)    
```


## Regression Tree: Implementation {.smaller}

**Ames Housing Dataset**

```{r, fig.align='center', fig.height=6, fig.width=8}
# variable importance

vip(object = tree_cv, num_features = 20, method = "model")
```

