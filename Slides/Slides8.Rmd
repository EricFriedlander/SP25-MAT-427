---
title: 'CMSC/LING/STAT 208: Machine Learning'
author: "Abhishek Chakraborty [Much of the content in these slides have been adapted from *ISLR2* by James et al. and *HOMLR* by Boehmke & Greenwell]"
output: ioslides_presentation
#output: pdf_document
#output: html_document
# output: beamer_presentation
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r, include=FALSE}
library(tidyverse)
# library(ggformula)
library(gridExtra)
# library(ISLR2)
# library(knitr)
# library(MASS)
library(caret)
# library(pROC)
library(recipes)
library(vip)
```

<style>
  .col2 {
    columns: 2 200px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 200px; /* chrome, safari */
    -moz-columns: 2 200px;    /* firefox */
  }
  .col3 {
    columns: 3 100px;
    -webkit-columns: 3 100px;
    -moz-columns: 3 100px;
  }
</style>



## Tree-Based Methods

* Involves **stratifying** or **segmenting** the predictor space into a number of simple regions.

* The set of splitting rules used to segment the predictor space can be summarized in a tree, thus, the name **decision tree** methods.

* Can be used for both classification and regression.

* Tree-based methods are simple and useful for interpretation, however, not the best in terms of prediction accuracy.

* Methods such as **bagging**, **random forests**, and **boosting** grow multiple trees and then combine their results.

<!-- ## Regression Trees -->

<!-- **Hitters dataset** -->

<!-- ```{r,echo=FALSE} -->
<!-- data("Hitters") -->
<!-- Hitters=na.omit(Hitters) -->
<!-- head(Hitters) -->
<!-- ``` -->

<!-- ## Regression Trees -->

<!-- **Hitters dataset** -->

<!-- log(Salary) is color-coded from low (blue, green) to high (yellow, red). -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '60%'} -->
<!-- knitr::include_graphics("EFT/SL_C8_1.png") -->
<!-- ``` -->

<!-- ## Regression Trees -->

<!-- **Hitters dataset** -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '60%'} -->
<!-- knitr::include_graphics("EFT/8.1.png") -->
<!-- ``` -->

<!-- ## Regression Trees -->

<!-- **Hitters dataset** -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->
<!-- knitr::include_graphics("EFT/8.2.png") -->
<!-- ``` -->



## Terminology for Trees

* Every split is considered to be a **node**.

* We refer to the first node at the top of the tree as the **root node** (this node contains all of the training data). 

* The final nodes at the bottom of the tree are called the **terminal nodes** or **leaves**.

<!-- The regions $R_1$, $R_2$, and $R_3$ are known as **terminal nodes** or **leaves**. -->

* Decision trees are typically drawn **upside down**, in the sense that the leaves are at the bottom of the tree.

* The points along the tree where the predictor space is split are referred to as **internal nodes**, that is, every node in between the **root node** and **terminal nodes** is referred to as an **internal node**.

* The segments of the trees that connect the nodes are known as **branches**.


## Terminology for Trees

```{r , echo=FALSE,  fig.align='center', fig.cap="Adapted from HMLR, Boehmke & Greenwell", out.width = '100%'}
knitr::include_graphics("EFT/tree2.jpg")
```


<!-- ## Interpretation of Trees -->

<!-- **Hitters dataset** -->

<!-- * **Years** is the most important factor in determining **Salary**, and players with less experience earn lower salaries than more experienced players. -->

<!-- * Given that a player is less experienced, the number of **Hits** that he made in the previous year seems to play little role in his **Salary**. -->

<!-- * But among players who have been in the major leagues for five or more years, the number of **Hits** made in the previous year does affect **Salary**, and players who made more **Hits** last year tend to have higher salaries. -->

<!-- * Surely an over-simplification, but compared to a regression model, it is easy to display, interpret and explain. -->


## Building a Tree

* First select the predictor $X_j$ and the cutpoint $s$ such that splitting the predictor space into the regions
$\{X|X_j < s\}$ and $\{X|X_j \ge s\}$ leads to the greatest possible reduction in $SSE$.

For any $j$ and $s$, define

$$R_1 = \{X|X_j < s\} \ \ \text{and} \ \ R_2 = \{X|X_j > s\}$$

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '67%'} -->
<!-- knitr::include_graphics("EFT/e8.2.png") -->
<!-- ``` -->

Find $j$ and $s$ that minimize

$$SSE = \displaystyle \sum_{i \in R_1}\left(y_i - \hat{y}_{R_1}\right)^2 + \sum_{i \in R_2}\left(y_i - \hat{y}_{R_2}\right)^2$$

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '67%'} -->
<!-- knitr::include_graphics("EFT/e8.3.png") -->
<!-- ``` -->

* Next, repeat the process, look for the best predictor and best cutpoint in order to split the data further.
However, this time, instead of splitting the entire predictor space, split one of the two previously identified regions.

* The process continues until a stopping criterion is reached; say, we may continue until no region contains more than five observations.





## Prediction

<!-- Building a tree involves two steps. -->

<!-- The steps involved are: -->

<!-- 1. Dividing the predictor space, that is, the set of possible values for $X_1, X_2, \ldots, X_p$ into $J$ distinct and non-overlapping regions, $R_1, R_2, \ldots, R_J$. -->

For every observation that falls into the region $R_j$, make the same prediction, which is 

  * the mean response of the training set observations in $R_j$ (for regression problems), 
  
  * majority vote response of the training set observations in $R_j$ (for classification problems).


## Building a Tree and Prediction

```{r , echo=FALSE,  fig.align='center', fig.cap="Adapted from HMLR, Boehmke & Greenwell", out.width = '100%'}
knitr::include_graphics("EFT/tree1.jpg")
```


## Building a Tree and Prediction

```{r , echo=FALSE,  fig.align='center', fig.cap="Adapted from HMLR, Boehmke & Greenwell", out.width = '100%'}
knitr::include_graphics("EFT/tree2.jpg")
```


## Building a Tree and Prediction

**Default Dataset**

```{r, fig.align='center', fig.height=6, fig.width=8, echo=FALSE}
library(ISLR2)

data("Default")

ggplot(data = Default) +
  geom_point(aes(x = balance, y = income, color = default), shape = c(3), alpha = 10)
```


## Building a Tree and Prediction

**Default Dataset**

```{r, fig.align='center', fig.height=6, fig.width=8, echo=FALSE}
library(rpart)
library(rpart.plot)
t <- rpart(default ~ balance + income, data = Default, method = "class")
rpart.plot(t)
```


## Building a Tree and Prediction

```{r , echo=FALSE,  fig.align='center', fig.cap="Adapted from ISLR, James et al.", out.width = '60%'}
knitr::include_graphics("EFT/8.3.png")
```


<!-- ## Building a Tree -->

<!-- * In theory, regions could have any shape. However, we -->
<!-- choose to divide the predictor space into **high-dimensional rectangles**, or **boxes**, for simplicity and for ease of interpretation of the resulting predictive model. -->

<!-- * **Objective**: Find boxes $R_1, R_2, \ldots R_J$ that minimize the $RSS$, -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '40%'} -->
<!-- knitr::include_graphics("EFT/e8.1.png") -->
<!-- ``` -->

<!-- where $\hat{y}_{R_j}$ is the mean response for the training observations within the $j^{th}$ box. -->


## Building a Tree

* It is computationally infeasible to consider every possible partition of the feature space into $J$ boxes.

* For this reason, we take a **top-down, greedy** approach known as **recursive binary splitting**.
    + **top-down** because it begins at the top of the tree and then successively splits the predictor space.
    + **greedy** because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.



<!-- ## Building a Tree -->

<!-- ```{r , echo=FALSE,  fig.align='center', fig.cap="Adapted from HMLR, Boehmke & Greenwell", out.width = '100%'} -->
<!-- knitr::include_graphics("tree2.jpg") -->
<!-- ``` -->




## Tree Pruning

The process described above may **overfit** the data.

```{r , echo=FALSE,  fig.align='center', out.width = '70%'}
knitr::include_graphics("EFT/pruning.png")
```



<!-- ## Tree Pruning -->

<!-- * The process described above may **overfit** the data. -->

<!-- * A smaller tree with fewer splits (that is, fewer regions $R_1,\ldots,R_J$) might lead to lower variance and better interpretation at the cost of a little bias. -->

<!-- * One possible alternative is to grow the tree only so long as the decrease in the $RSS$ due to each split exceeds some (high) threshold. -->

<!-- * This strategy will result in smaller trees, but is too short-sighted: a seemingly worthless split early on in the tree might be followed by a very good split, that is, a split that leads to a large reduction in $RSS$ later on. -->



## Tree Pruning

<!-- * A better strategy is **pruning**. -->

* Grow a very large tree, and then **prune** it back to obtain a **subtree**.

<!-- $T_0$ -->

* The technique uses is known as **cost complexity pruning** (also known as **weakest link pruning**).

* Consider a sequence of trees indexed by $\alpha$. For each $\alpha$, consider the tree that minimizes

$$SSE + \alpha \ |T|$$


<!-- $T \subset T_0$ -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->
<!-- knitr::include_graphics("EFT/e8.4.png") -->
<!-- ``` -->

* Choose optimal $\alpha$ by CV.


<!-- ## Building an Optimal Tree -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '90%'} -->
<!-- knitr::include_graphics("EFT/algo8.1.png") -->
<!-- ``` -->

<!-- ## Building an Optimal Tree -->

<!-- **Hitters dataset** -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->
<!-- knitr::include_graphics("EFT/8.4.png") -->
<!-- ``` -->

<!-- ## Building an Optimal Tree -->

<!-- **Hitters dataset** -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '90%'} -->
<!-- knitr::include_graphics("EFT/8.5.png") -->
<!-- ``` -->

## Regression Tree: Implementation

**Ames Housing Dataset**

```{r}
ames <- readRDS("AmesHousing.rds")   # load dataset
```

```{r}
# reorder levels of 'Overall_Qual'
ames$Overall_Qual <- factor(ames$Overall_Qual, levels = c("Very_Poor", "Poor", "Fair", "Below_Average", 
                                                          "Average", "Above_Average", "Good", "Very_Good", 
                                                          "Excellent", "Very_Excellent"))
```

```{r}
# split data

set.seed(050924)   # set seed

train_index <- createDataPartition(y = ames$Sale_Price, p = 0.7, list = FALSE)   # consider 70-30 split

ames_train <- ames[train_index,]   # training data

ames_test <- ames[-train_index,]   # test data
```


## Regression Tree: Implementation

**Ames Housing Dataset**

```{r}
# create recipe and blueprint, prepare and apply blueprint

set.seed(050924)   # set seed

ames_recipe <- recipe(Sale_Price ~ ., data = ames_train)   # set up recipe

blueprint <- ames_recipe %>%
  step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %>%   # filter out zv/nzv predictors
  step_impute_mean(Gr_Liv_Area) %>%                                    # impute missing entries
  step_integer(Overall_Qual) %>%                                       # numeric conversion of levels of the predictors
  step_center(all_numeric(), -all_outcomes()) %>%                      # center (subtract mean) all numeric predictors
  step_scale(all_numeric(), -all_outcomes()) %>%                       # scale (divide by standard deviation) all numeric predictors
  step_other(Neighborhood, threshold = 0.01, other = "other") %>%      # lumping required predictors
  step_dummy(all_nominal(), one_hot = FALSE)                           # one-hot/dummy encode nominal categorical predictors


prepare <- prep(blueprint, data = ames_train)    # estimate feature engineering parameters based on training data


baked_train <- bake(prepare, new_data = ames_train)   # apply the blueprint to training data

baked_test <- bake(prepare, new_data = ames_test)    # apply the blueprint to test data
```


## Regression Tree: Implementation

**Ames Housing Dataset**

Implement CV to tune the hyperparameter.

```{r}
set.seed(050924)   # set seed

cv_specs <- trainControl(method = "repeatedcv", number = 5, repeats = 5)   # CV specifications


library(rpart)   # for trees

tree_cv <- train(blueprint,
                 data = ames_train,
                 method = "rpart",  
                 trControl = cv_specs,
                 tuneLength = 20,                   # considers a grid of 20 possible tuning parameter values
                 metric = "RMSE")
```


```{r}
# results from the CV procedure

tree_cv$bestTune    # optimal hyperparameter

min(tree_cv$results$RMSE)   # optimal CV RMSE
```


## Regression Tree: Implementation {.smaller}

**Ames Housing Dataset**

Results from the CV procedure.

```{r, fig.align='center', fig.height=6, fig.width=8}
ggplot(tree_cv)   
```


## Regression Tree: Implementation

**Ames Housing Dataset**

```{r}
# build final model

final_model <- rpart(formula = Sale_Price ~ .,
                     data = baked_train,
                     cp = tree_cv$bestTune$cp,
                     xval = 0,                 # no further CV
                     method = "anova")         # for regression
```


```{r}
# obtain predictions and test set RMSE

final_model_preds <- predict(object = final_model, newdata = baked_test, type = "vector")    # obtain test set predictions

sqrt(mean((final_model_preds - baked_test$Sale_Price)^2))   # calculate test set RMSE
```


## Regression Tree: Implementation 

**Ames Housing Dataset**

```{r, fig.align='center', fig.height=6, fig.width=8}
library(rpart.plot)
rpart.plot(final_model)    
```


## Regression Tree: Implementation {.smaller}

**Ames Housing Dataset**

```{r, fig.align='center', fig.height=6, fig.width=8}
# variable importance

vip(object = tree_cv, num_features = 20, method = "model")
```


## Regression Tree: Implementation

**Ames Housing Dataset**

```{r}
# build full grown tree (no pruning)

final_model_no_prune <- rpart(formula = Sale_Price ~ .,
                              data = baked_train,
                              cp = 0,                   # no pruning
                              xval = 0,                 # no CV
                              method = "anova")         # for regression
```


```{r}
# obtain predictions and test set RMSE

final_model_no_prune_preds <- predict(object = final_model_no_prune, newdata = baked_test, type = "vector")    # test set predictions

sqrt(mean((final_model_no_prune_preds - baked_test$Sale_Price)^2))   # calculate test set RMSE
```


## Regression Tree: Implementation 

**Ames Housing Dataset**

```{r, fig.align='center', fig.height=6, fig.width=8}
rpart.plot(final_model_no_prune)    
```


<!-- ## Regression Tree: Boston Data Example  {.smaller} -->


<!-- ```{r} -->
<!-- library(MASS) -->

<!-- data("Boston")   # load dataset -->

<!-- set.seed(05172022) -->

<!-- ## investigate dataset -->

<!-- str(Boston)  # check variable types -->
<!-- ``` -->


<!-- ## Regression Tree: Boston Data Example  {.smaller} -->

<!-- ```{r} -->
<!-- summary(Boston) -->
<!-- ``` -->

<!-- ## Regression Tree: Boston Data Example  {.smaller} -->


<!-- ```{r} -->
<!-- sum(is.na(Boston))   # check for missing values -->

<!-- nearZeroVar(Boston, saveMetrics = TRUE)   # check for zv/nzv features -->
<!-- ``` -->


<!-- ## Regression Tree: Boston Data Example   -->


<!-- ```{r} -->
<!-- # split the data into training and test sets -->

<!-- train_index <- createDataPartition(Boston$medv, p = 0.8, list = FALSE)   # 80-20 split -->

<!-- Boston_train <- Boston[train_index, ]   # training set -->

<!-- Boston_test <- Boston[-train_index, ]   # test set -->


<!-- # create feature preprocessing blueprint -->

<!-- blueprint <- recipe(medv ~ ., data = Boston_train) %>% -->
<!--   step_normalize(all_numeric_predictors()) -->

<!-- prepare <- prep(blueprint, training = Boston_train) -->

<!-- baked_train <- bake(prepare, new_data = Boston_train) -->

<!-- baked_test <- bake(prepare, new_data = Boston_test) -->
<!-- ``` -->


<!-- ## Regression Tree: Boston Data Example   -->


<!-- ```{r} -->
<!-- # implement CV -->

<!-- cv_specs <- trainControl(method = "repeatedcv", number = 5, repeats = 5) -->

<!-- tree_cv <- train(blueprint, -->
<!--                  data = Boston_train, -->
<!--                  method = "rpart", -->
<!--                  trControl = cv_specs, -->
<!--                  tuneLength = 20,     # considers a grid of 20 possible tuning parameter values -->
<!--                  metric = "RMSE") -->

<!-- tree_cv$bestTune   # optimal tuning parameter -->

<!-- min(tree_cv$results$RMSE)   # CV RMSE -->
<!-- ``` -->

<!-- ## Regression Tree: Boston Data Example   -->

<!-- ```{r} -->
<!-- tree_cv$finalModel$variable.importance   # importance of features -->
<!-- ``` -->


<!-- ## Regression Tree: Boston Data Example   -->


<!-- ```{r} -->
<!-- # fit final optimal model -->

<!-- tree_fit <- rpart(medv ~ ., -->
<!--                   data = baked_train, -->
<!--                   cp = tree_cv$bestTune$cp, -->
<!--                   xval = 0, -->
<!--                   method = "anova") -->


<!-- preds_tree <- predict(tree_fit, newdata = baked_test)   # obtain predictions on test set -->

<!-- sqrt(mean((preds_tree - baked_test$medv)^2))   # test RMSE -->
<!-- ``` -->

<!-- ## Regression Tree: Boston Data Example   -->


<!-- ```{r, fig.align='center'} -->
<!-- rpart.plot(tree_fit, cex = 0.75) -->
<!-- ``` -->


<!-- ## Regression Tree: Boston Data Example   -->


<!-- ```{r} -->
<!-- # fit full grown tree (with no pruning) for comparison -->

<!-- tree_fit1 <- rpart(medv ~ ., -->
<!--                    data = baked_train, -->
<!--                    cp = 0,  -->
<!--                    xval = 0, -->
<!--                    method = "anova") -->
<!-- ``` -->


<!-- ## Regression Tree: Boston Data Example   -->


<!-- ```{r, fig.align='center'} -->
<!-- rpart.plot(tree_fit1, cex = 0.75) -->
<!-- ``` -->



<!-- ## Classification Trees -->

<!-- * Very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one. -->

<!-- * For a classification tree, we predict that each observation belongs to the **most commonly occurring class** of training observations in the region to which it belongs. -->


<!-- * However, classification error is not sufficiently sensitive for tree-growing. -->

<!-- ## Classification Trees -->

<!-- * Another measure is the **Gini Index** -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '50%'} -->
<!-- knitr::include_graphics("EFT/e8.6.png") -->
<!-- ``` -->

<!-- measures the total variance across the $K$ classes. It is referred to as a measure of **node purity**. -->

<!-- * An alternative to the Gini index is **entropy** -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '50%'} -->
<!-- knitr::include_graphics("EFT/e8.7.png") -->
<!-- ``` -->

<!-- ## Classification Trees -->

<!-- **Heart dataset** -->

<!-- ```{r, echo=FALSE} -->
<!-- Heart=read.csv("Heart.csv") -->
<!-- Heart$X=NULL -->
<!-- head(Heart) -->
<!-- ``` -->

<!-- ## Classification Trees -->

<!-- **Heart dataset** -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '100%'} -->
<!-- knitr::include_graphics("EFT/8.6a.png") -->
<!-- ``` -->

<!-- ## Classification Trees -->

<!-- **Heart dataset** -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '100%'} -->
<!-- knitr::include_graphics("EFT/8.6b.png") -->
<!-- ``` -->

<!-- ## Trees vs Linear Models -->

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '70%'} -->
<!-- knitr::include_graphics("EFT/8.7.png") -->
<!-- ``` -->

## Trees

**Advantages**

* Easy to explain.

* Closely mirror human decision-making.

* Can be displayed graphically, and are easily interpreted by non-experts.

* Handle qualitative predictors without creating dummy variables. Does not require standardization of predictors.

**Disadvantages**

* Do not have same level of prediction accuracy.

* Can be very non-robust.


## Regression Tree: Implementation

**Ames Housing Dataset** (minimal feature engineering)

```{r}
# create new blueprint (minimal feature engineering), prepare and apply blueprint

set.seed(050924)   # set seed

ames_recipe <- recipe(Sale_Price ~ ., data = ames_train)   # set up recipe

blueprint_minimal <- ames_recipe %>%
  step_impute_mean(Gr_Liv_Area)                                    # impute missing entries


prepare_minimal <- prep(blueprint_minimal, data = ames_train)    # estimate feature engineering parameters based on training data


baked_train_minimal <- bake(prepare_minimal, new_data = ames_train)   # apply the blueprint to training data

baked_test_minimal <- bake(prepare_minimal, new_data = ames_test)    # apply the blueprint to test data
```


## Regression Tree: Implementation

**Ames Housing Dataset**

Implement CV to tune the hyperparameter.

```{r}
set.seed(050924)   # set seed

cv_specs <- trainControl(method = "repeatedcv", number = 5, repeats = 5)   # CV specifications


tree_cv_min_fe <- train(blueprint_minimal,
                        data = ames_train,
                        method = "rpart",  
                        trControl = cv_specs,
                        tuneLength = 20,                   # considers a grid of 20 possible tuning parameter values
                        metric = "RMSE")
```


```{r}
# results from the CV procedure

tree_cv_min_fe$bestTune    # optimal hyperparameter

min(tree_cv_min_fe$results$RMSE)   # optimal CV RMSE

# doing feature engineering seems to result in a slightly better performance
```


<!-- ## Regression Tree: Implementation -->

<!-- **Ames Housing Dataset** -->

<!-- ```{r} -->
<!-- # build final model -->

<!-- final_model_min_fe <- rpart(formula = Sale_Price ~ ., -->
<!--                             data = baked_train_new, -->
<!--                             cp = tree_cv_min_fe$bestTune$cp, -->
<!--                             xval = 0,                 # no CV -->
<!--                             method = "anova")         # for regression -->
<!-- ``` -->


<!-- ```{r} -->
<!-- # obtain predictions and test set RMSE -->

<!-- final_model_min_fe_preds <- predict(final_model_min_fe, newdata = baked_test_new)    # obtain test set predictions -->

<!-- sqrt(mean((final_model_min_fe_preds - baked_test_new$Sale_Price)^2))   # calculate test set RMSE -->
<!-- ``` -->


## Regression Tree: Implementation

**Ames Housing Dataset**


```{r, fig.align='center', fig.height=6, fig.width=8}
# variable importance

vip(object = tree_cv_min_fe, num_features = 20, method = "model")          
```



## Classification Trees

* Still use **recursive binary splitting** to grow a classification tree.

* $SSE$ can be replaced by 

    - **classification error rate**, the fraction of the training observations in that region that do not belong to the most common class.
    
    $$E = 1 - \max_k \left(\hat{p}_{mk}\right)$$
    
  - **Gini index**,  a measure of node purity—a small value indicates that a node contains predominantly observations from a single class.
    
    
  $$G = \displaystyle \sum_{k=1}^{K} \hat{p}_{mk}\left(1-\hat{p}_{mk}\right)$$
    
    

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '50%'} -->
<!-- knitr::include_graphics("EFT/e8.5.png") -->
<!-- ``` -->


Here $\hat{p}_{mk}$ represents the proportion of training observations in the $m^{th}$ region that are from the $k^{th}$ class.




<!-- ## <span style="color:blue">Your Turn!!!</span> {.smaller} -->

<!-- You will work with the `iris` dataset which contains measurements in centimeters of four variables for 50 flowers from each of 3 species of iris: setosa, versicolor, and virginica. Please load the dataset using the following code.  -->

<!-- ```{r} -->
<!-- data(iris)    # load dataset -->
<!-- ``` -->

<!-- We are interested in predicting `Species` using the rest of the variables in the dataset. Compare the performance (in terms of **Accuracy**) of the following models: -->

<!-- * A logistic regression model; -->

<!-- * A $K$-NN model with optimal $K$ chosen by CV; -->

<!-- * A classification tree with optimal hyperparameter chosen by CV. Use `tuneLength = 20`. -->

<!-- * A classification tree with minimal feature engineering. Use CV to choose the optimal hyperparameter. Use `tuneLength = 20`. -->


<!-- **Perform the following tasks.** -->

<!-- * Investigate the dataset and complete any necessary tasks.  -->

<!-- * Split the data into training and test sets (75-25). -->

<!-- * Perform required data preprocessing and create the blueprint. If using `step_dummy()`, set `one_hot = FALSE`. Prepare the blueprint on the training data. Obtain the modified training and test datasets.  -->

<!-- * Implement 10-fold CV repeated 5 times for each of the models above.  -->

<!-- * Report the optimal CV Accuracy of each model. Report the optimal hyperparameters for each model. Which model performs best in this situation?  -->

<!-- * Build the final model. Obtain class label predictions on the test set. Create the corresponding confusion matrix and report the test set accuracy. Based on your optimal final model, consult the help page for either `predict.knn3` or `predict.rpart` functions. -->

<!-- ## <span style="color:blue">Your Turn!!!</span> {.smaller} -->

<!-- ```{r} -->
<!-- glimpse(iris)   # all features are numerical -->
<!-- ``` -->

<!-- ```{r} -->
<!-- summary(iris)   # summary of variables -->
<!-- ``` -->

<!-- ```{r} -->
<!-- sum(is.na(iris))  # no missing entries -->
<!-- ``` -->



<!-- ## <span style="color:blue">Your Turn!!!</span> -->

<!-- ```{r} -->
<!-- set.seed(050924)   # set seed -->

<!-- # split the data into training and test sets -->

<!-- index <- createDataPartition(iris$Species, p = 0.75, list = FALSE) -->

<!-- iris_train <- iris[index, ] -->

<!-- iris_test <- iris[-index, ] -->
<!-- ``` -->


<!-- ```{r} -->
<!-- nearZeroVar(iris_train, saveMetrics = TRUE)  # no zv/nzv features -->
<!-- ``` -->



<!-- ## <span style="color:blue">Your Turn!!!</span> -->

<!-- ```{r} -->
<!-- set.seed(050924)   # set seed -->

<!-- # create recipe and blueprint, prepare and apply blueprint -->

<!-- blueprint <- recipe(Species ~ ., data = iris_train) %>% -->
<!--   step_normalize(all_predictors())               # center and scale numeric predictors, in this case, all predictors -->

<!-- prepare <- prep(blueprint, training = iris_train) -->

<!-- baked_train <- bake(prepare, new_data = iris_train) -->

<!-- baked_test <- bake(prepare, new_data = iris_test) -->
<!-- ``` -->


<!-- ## <span style="color:blue">Your Turn!!!</span> -->

<!-- ```{r} -->
<!-- set.seed(050924)   # set seed -->

<!-- cv_specs <- trainControl(method = "repeatedcv", number = 10, repeats = 5)   # CV specifications -->
<!-- ``` -->

<!-- ```{r, eval = FALSE} -->
<!-- logistic_cv <- train(blueprint, -->
<!--                      data = iris_train, -->
<!--                      method = "glm", -->
<!--                      family = "binomial", -->
<!--                      trControl = cv_specs, -->
<!--                      metric = "Accuracy") -->

<!-- # The code above will throw an error since this is a 3-class classification problem and -->
<!-- # logistic regression (with family = binomial) works for a binary (2-class) problem. -->
<!-- ``` -->


<!-- ## <span style="color:blue">Your Turn!!!</span> -->

<!-- ```{r} -->
<!-- set.seed(050924)   # set seed -->

<!-- # CV for KNN -->
<!-- k_grid <- expand.grid(k = seq(1, 101, by = 10)) -->

<!-- knn_cv <- train(blueprint, -->
<!--                 data = iris_train, -->
<!--                 method = "knn", -->
<!--                 trControl = cv_specs, -->
<!--                 tuneGrid = k_grid, -->
<!--                 metric = "Accuracy") -->


<!-- # CV with tree -->
<!-- tree_cv <- train(blueprint, -->
<!--                  data = iris_train, -->
<!--                  method = "rpart", -->
<!--                  trControl = cv_specs, -->
<!--                  tuneLength = 20, -->
<!--                  metric = "Accuracy") -->


<!-- # CV with tree (minimal feature engineering, no engineering in this case) -->
<!-- tree_cv_min_fe <- train(Species ~ ., -->
<!--                         data = iris_train, -->
<!--                         method = "rpart", -->
<!--                         trControl = cv_specs, -->
<!--                         tuneLength = 20, -->
<!--                         metric = "Accuracy") -->
<!-- ``` -->


<!-- ## <span style="color:blue">Your Turn!!!</span> -->

<!-- ```{r} -->
<!-- # optimal CV Accuracies -->

<!-- max(knn_cv$results$Accuracy)     # for KNN -->

<!-- max(tree_cv$results$Accuracy)     # for classification tree -->

<!-- max(tree_cv_min_fe$results$Accuracy)     # for classification tree with minimal feature engineering -->
<!-- ``` -->


<!-- ## <span style="color:blue">Your Turn!!!</span> -->

<!-- ```{r} -->
<!-- # optimal hyperparameters -->

<!-- knn_cv$bestTune$k    # for KNN -->

<!-- tree_cv$bestTune$cp    # for classification tree -->

<!-- tree_cv_min_fe$bestTune$cp    # for classification tree with minimal feature engineering -->
<!-- ``` -->


<!-- ## <span style="color:blue">Your Turn!!!</span> {.smaller} -->

<!-- ```{r} -->
<!-- # final model -->

<!-- final_model <- knn3(Species ~ ., data = baked_train, k = knn_cv$bestTune$k) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # obtain probability and class label predictions -->

<!-- final_model_prob_preds <- predict(final_model, newdata = baked_test, type = "prob")   # probability predictions -->

<!-- final_model_class_preds <- predict(final_model, newdata = baked_test, type = "class")   # class label predictions -->

<!-- confusionMatrix(data = final_model_class_preds, reference = baked_test$Species) -->
<!-- ``` -->







## Ensemble Methods

Single regression or classification trees usually have poor predictive performance.

**Ensemble Methods** use a collection of multiple trees to improve the predictive performance at the cost of interpretability. 

* Bagging

* Random Forests

* Boosting

```{r , echo=FALSE,  fig.cap="Adapted from ISLR, James et al.", fig.align='center', out.width = '70%'}
knitr::include_graphics("EFT/2.7.png")
```



## Bagging

* **Bootstrap aggregation** or **bagging** is a general-purpose procedure for reducing the variance of a statistical learning method.

* **Idea**: Build multiple trees and average their results.

* **Result**: Given a set of $n$ independent observations (random variables) $Z_1, \ldots, Z_n$, each with variance $\sigma^2$, the variance of the mean/average $\bar{Z} = \displaystyle \dfrac{Z_1 + Z_2 + \cdots + Z_n}{n}$ of the observations is $\sigma^2/n$.

In other words, **averaging a set of observations reduces variance**.

* In reality, we do not have multiple training datasets.


## Bagging

**Bootstrapping**

```{r , echo=FALSE, fig.cap="Adapted from ISLR, James et al.", fig.align='center', out.width = '70%'}
knitr::include_graphics("EFT/bootstrapping.PNG")
```

## Bagging

* Take repeated bootstrap samples (say $B$) from the original (single) available dataset.

* Build a tree on each bootstrap sample and obtain predictions $\hat{f}^{*b}(x), \ b=1, 2, \ldots, B$.

* Average all the predictions.

```{r , echo=FALSE,  fig.align='center', out.width = '40%'}
knitr::include_graphics("EFT/e8.95b.png")
```

* Individual trees are grown deep and are not pruned. They have high variance, but low bias.

* For classification trees, take **majority vote**: the overall prediction is the most commonly occurring class among the $B$ predictions.


## Out-of-Bag Error Estimation

* A straightforward way to estimate the test error of a bagged model, without performing CV.

* It can be shown that on average, each bagged tree (constructed on each bootstrap sample) makes use of around two-thirds of the observations. 

<!-- (Exercise 2 from Chapter 5) -->

* Remaining one-third observations not used train a bagged tree are referred to as **out-of-bag (OOB)** observations.

* For $i^{th}$ observation, use the trees in which that observation was OOB. This will yield around $B/3$ predictions for the $i^{th}$ observation. Take their average to obtain a single prediction.

* Equivalent to LOOCV if $B$ is large.


## Variable Importance Measures

* Bagging improves prediction accuracy at the expense of interpretability.

* However, one can still obtain an overall summary of the importance of each predictor.


<!-- ## Variable Importance Measures -->

* To measure feature importance, the reduction in the loss function (e.g., RSS) attributed to each variable at each split is tabulated. In some instances, a single variable could be used multiple times in a tree; consequently, the total reduction in the loss function across all splits by a variable are summed up and used as the total feature importance.

* A large value indicates an important predictor.

<!-- ```{r , echo=FALSE,  fig.align='center', out.width = '50%'} -->
<!-- knitr::include_graphics("EFT/8.9.png") -->
<!-- ``` -->


## Bagging: Implementation {.smaller}

**Ames Housing Dataset**

```{r, echo=FALSE}
ames <- readRDS("AmesHousing.rds")   # load dataset

ames$Overall_Qual <- factor(ames$Overall_Qual, levels = c("Very_Poor", "Poor", "Fair", "Below_Average",
                                                  "Average", "Above_Average", "Good", "Very_Good",
                                                  "Excellent", "Very_Excellent"))
```

```{r, echo=FALSE}
# split data

set.seed(051424)   # set seed

index <- createDataPartition(y = ames$Sale_Price, p = 0.7, list = FALSE)   # consider 70-30 split

ames_train <- ames[index,]   # training data

ames_test <- ames[-index,]   # test data
```

```{r, echo=FALSE}
# create recipe and blueprint, prepare and apply blueprint

set.seed(051424)   # set seed

ames_recipe <- recipe(Sale_Price ~ ., data = ames_train)   # set up recipe

blueprint <- ames_recipe %>%
  step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %>%   # filter out zv/nzv predictors
  step_impute_mean(Gr_Liv_Area) %>%                                    # impute missing entries
  step_integer(Overall_Qual) %>%                                       # numeric conversion of levels of the predictors
  step_center(all_numeric(), -all_outcomes()) %>%                      # center (subtract mean) all numeric predictors
  step_scale(all_numeric(), -all_outcomes()) %>%                       # scale (divide by standard deviation) all numeric predictors
  step_other(Neighborhood, threshold = 0.01, other = "other") %>%      # lumping required predictors
  step_dummy(all_nominal(), one_hot = FALSE)                           # one-hot/dummy encode nominal categorical predictors


prepare <- prep(blueprint, data = ames_train)    # estimate feature engineering parameters based on training data


baked_train <- bake(prepare, new_data = ames_train)   # apply the blueprint to training data

baked_test <- bake(prepare, new_data = ames_test)    # apply the blueprint to test data
```

Data splitting and feature engineering has been done in the previous slides.

```{r}
set.seed(051424)   # set seed

library(ipred)   # for bagging

bag_fit <- bagging(formula = Sale_Price ~ ., 
                   data = baked_train,
                   nbagg = 500,                             # number of trees to grow (bootstrap samples) usually 500
                   coob = TRUE,                             # yes to computing OOB error estimate
                   control = rpart.control(minsplit = 2,    # split a node if at least 2 observations present
                                           cp = 0,          # no pruning (let the trees grow tall)
                                           xval = 0))       # no CV 
```                  

```{r}
bag_fit   # results of bagging 
```

```{r}
bag_fit$err   # OOB RMSE estimate
```


## Bagging: Implementation

**Ames Housing Dataset**

CV with bagging <span style="color:red">(NOT recommended since computationally expensive)</span>

```{r, eval=FALSE}
set.seed(051424)   # set seed

cv_specs <- trainControl(method = "repeatedcv", number = 5, repeats = 1)   # CV specifications

library(ipred)
library(e1071)

bagging_cv <- train(blueprint,
                    data = ames_train,
                    method = "treebag",  
                    trControl = cv_specs,
                    nbagg = 500,                   
                    control = rpart.control(minsplit = 2, cp = 0),    
                    metric = "RMSE")
```


<!-- ```{r, echo=FALSE} -->
<!-- data("Boston") -->

<!-- train_index <- createDataPartition(Boston$medv, p = 0.8, list = FALSE) -->
<!-- Boston_train <- Boston[train_index, ] -->
<!-- Boston_test <- Boston[-train_index, ] -->

<!-- blueprint <- recipe(medv ~ ., data = Boston_train) %>% -->
<!--   step_normalize(all_numeric_predictors()) -->

<!-- prepare <- prep(blueprint, training = Boston_train) -->

<!-- baked_train <- bake(prepare, new_data = Boston_train) -->
<!-- baked_test <- bake(prepare, new_data = Boston_test) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- bag_fit <- bagging(medv ~ .,  -->
<!--                   data = baked_train, -->
<!--                   nbagg = 500,   # number of trees to grow (bootstrap replications) -->
<!--                   coob = TRUE,   # yes to computing OOB error estimate -->
<!--                   control = rpart.control(minsplit = 2, cp = 0, xval = 0))  # details of each tree -->


<!-- bag_fit -->
<!-- ``` -->

## Bagging: Implementation

```{r}
# obtain predictions on the test set

final_model_preds <- predict(object = bag_fit, newdata = baked_test)     # use 'type = "class"' for classification trees

sqrt(mean((final_model_preds - baked_test$Sale_Price)^2))   # test set RMSE
```


```{r, eval=FALSE}
# variable importance

imp <- varImp(bag_fit)      # look at the object created
```


## Bagging: Disadvantages

* Bagging improves the prediction accuracy for high variance (and low bias) models at the expense of interpretability and computational speed.

* However, although the model building steps are independent, the trees in bagging are not completely independent of each other since all the original features are considered at every split of every tree. Rather, trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to any underlying strong relationships.

* This characteristic is known as **tree correlation** and prevents bagging from further reducing the variance of the individual models. **Random forests** extend and improve upon bagged decision trees by reducing this correlation and thereby improving the accuracy of the overall ensemble.


## Bagging: Disadvantages

**Tree Correlation in Bagging**

```{r , echo=FALSE,  fig.align='center', fig.cap="Adapted from HMLR, Boehmke & Greenwell", out.width = '100%'}
knitr::include_graphics("EFT/tcBagging.jpg")
```



## Random Forests

* Provide an improvement over bagged trees by reducing the variance further (by **decorrelating**) when we average the trees.

* As in bagging, we build a number of decision trees on bootstrapped training samples.

* For each tree, each time a split is considered, a **random selection of $m$ predictors** is chosen (split candidates) from the full set of $p$ predictors. The split is allowed to use only one of those $m$ predictors. Note that in bagging, each split for each tree considers all $p$ predictors as split candidates.

* A fresh sample of $m$ predictors is taken at each split. Typical default values are $m = p/3$ (regression) and $m = \sqrt{p}$ (classification) but this should be considered a tuning parameter, to be chosen by CV.


## Random Forests: Implementation  {.smaller}

**Ames Housing Dataset**

```{r, echo=FALSE}
ames <- readRDS("AmesHousing.rds")   # load dataset
```

```{r, echo=FALSE}
# reorder levels of 'Overall_Qual'
ames$Overall_Qual <- factor(ames$Overall_Qual, levels = c("Very_Poor", "Poor", "Fair", "Below_Average", 
                                                          "Average", "Above_Average", "Good", "Very_Good", 
                                                          "Excellent", "Very_Excellent"))
```

```{r, echo=FALSE}
# split data

set.seed(051424)   # set seed

index <- createDataPartition(y = ames$Sale_Price, p = 0.7, list = FALSE)   # consider 70-30 split

ames_train <- ames[index,]   # training data

ames_test <- ames[-index,]   # test data
```

```{r, echo=FALSE}
# create recipe and blueprint, prepare and apply blueprint

set.seed(051424)   # set seed

ames_recipe <- recipe(Sale_Price ~ ., data = ames_train)   # set up recipe

blueprint <- ames_recipe %>%
  step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %>%   # filter out zv/nzv predictors
  step_impute_mean(Gr_Liv_Area) %>%                                    # impute missing entries
  step_integer(Overall_Qual) %>%                                       # numeric conversion of levels of the predictors
  step_center(all_numeric(), -all_outcomes()) %>%                      # center (subtract mean) all numeric predictors
  step_scale(all_numeric(), -all_outcomes()) %>%                       # scale (divide by standard deviation) all numeric predictors
  step_other(Neighborhood, threshold = 0.01, other = "other") %>%      # lumping required predictors
  step_dummy(all_nominal(), one_hot = FALSE)                           # one-hot/dummy encode nominal categorical predictors


prepare <- prep(blueprint, data = ames_train)    # estimate feature engineering parameters based on training data


baked_train <- bake(prepare, new_data = ames_train)   # apply the blueprint to training data

baked_test <- bake(prepare, new_data = ames_test)    # apply the blueprint to test data
```


Data splitting and feature engineering has been done in the previous slides.


```{r}
set.seed(051424)   # set seed

cv_specs <- trainControl(method = "cv", number = 5)   # CV specifications


library(ranger)
library(e1071)


param_grid <- expand.grid(mtry = seq(1, 30, 1),     # sequence of 1 to at least half the number of predictors
                          splitrule = "variance",   # use "gini" for classification
                          min.node.size = 2)        # for each tree

rf_cv <- train(blueprint,
               data = ames_train,
               method = "ranger",
               trControl = cv_specs,
               tuneGrid = param_grid,
               metric = "RMSE")

rf_cv$bestTune$mtry   # optimal tuning parameter

min(rf_cv$results$RMSE)   # optimal CV RMSE
```


## Random Forests: Implementation

**Ames Housing Dataset**

```{r}
# fit final model

final_model <- ranger(formula = Sale_Price ~ .,
                      data = baked_train,
                      num.trees = 500,
                      mtry = rf_cv$bestTune$mtry,
                      splitrule = "variance",
                      min.node.size = 2, 
                      importance = "impurity")
```

```{r}
# obtain predictions on the test set

final_model_preds <- predict(object = final_model, data = baked_test, type = "response")  # predictions on test set

sqrt(mean((final_model_preds$predictions - baked_test$Sale_Price)^2))  # test set RMSE
```


## Random Forests: Implementation {.smaller}

**Ames Housing Dataset**

```{r}
# variable importance

vip(final_model, num_features = 20)      # top 20 most important features
```



## Gradient Boosting

Like bagging and random forests, boosting involves combining a large number of decision trees. However, the main idea of boosting is to add new models to the ensemble **sequentially**. 

Each model in the process is a weak model, referred to as the **base learner**. The idea behind boosting is that each model in the sequence slightly improves upon the performance of the previous one, thereby **learning slowly**.

At each step of the process, we fit a tree to the residuals from the previous model, rather than the outcome $Y$, as the response. The final model is a stagewise additive model of $B$ individual trees.

$$\hat{f}(x) = \displaystyle \sum_{b=1}^{B} \lambda \ \hat{f}^b(x)$$

The process is called **gradient boosting** since it uses the **gradient descent** algorithm to minimize the loss function.


## Gradient Boosting


## Gradient Boosting

```{r, echo=FALSE, fig.align='center', fig.height=6, fig.width=8}
# Simulate sine wave data
set.seed(1112)  # for reproducibility
df <- tibble::tibble(
  x = seq(from = 0, to = 2 * pi, length = 1000),
  y = sin(x) + rnorm(length(x), sd = 0.5),
  truth = sin(x)
)

# Function to boost `rpart::rpart()` trees
rpartBoost <- function(x, y, data, num_trees = 100, learn_rate = 0.1, tree_depth = 6) {
  x <- data[[deparse(substitute(x))]]
  y <- data[[deparse(substitute(y))]]
  G_b_hat <- matrix(0, nrow = length(y), ncol = num_trees + 1)
  r <- y
  for(tree in seq_len(num_trees)) {
    g_b_tilde <- rpart(r ~ x, control = list(cp = 0, maxdepth = tree_depth))
    g_b_hat <- learn_rate * predict(g_b_tilde)
    G_b_hat[, tree + 1] <- G_b_hat[, tree] + matrix(g_b_hat)
    r <- r - g_b_hat
    colnames(G_b_hat) <- paste0("tree_", c(0, seq_len(num_trees)))
  }
  cbind(df, as.data.frame(G_b_hat)) %>%
    gather(tree, prediction, starts_with("tree")) %>%
    mutate(tree = stringr::str_extract(tree, "\\d+") %>% as.numeric())
}

# Plot boosted tree sequence
rpartBoost(x, y, data = df, num_trees = 2^10, learn_rate = 0.05, tree_depth = 1) %>%
  filter(tree %in% c(0, 2^c(0:10))) %>%
  ggplot(aes(x, prediction)) +
    ylab("y") +
    geom_point(data = df, aes(x, y), alpha = .1) +
    geom_line(data = df, aes(x, truth), color = "cyan") +
    geom_line(colour = "red", size = 1) +
    facet_wrap(~ tree, nrow = 3) +
    theme_bw()

```


## Gradient Boosting 

The hyperparameters (to be tuned by CV) include:

* **Number of trees**: Having too many trees can overfit the training data.
\
\

* **Shrinkage** ($\lambda$): Determines the contribution of each tree to the final model and controls how quickly the algorithm learns.
\
\

* **Tree Depth**: Controls the depth of the individual trees.  
\
\

* **Minimum number of observations in terminal nodes**: Also, controls the complexity of each tree.

## Gradient Boosting: Implementation  {.smaller}

**Ames Housing Dataset**

```{r, echo=FALSE}
ames <- readRDS("AmesHousing.rds")   # load dataset
```

```{r, echo=FALSE}
# reorder levels of 'Overall_Qual'
ames$Overall_Qual <- factor(ames$Overall_Qual, levels = c("Very_Poor", "Poor", "Fair", "Below_Average", 
                                                          "Average", "Above_Average", "Good", "Very_Good", 
                                                          "Excellent", "Very_Excellent"))
```

```{r, echo=FALSE}
# split data

set.seed(051624)   # set seed

index <- createDataPartition(y = ames$Sale_Price, p = 0.7, list = FALSE)   # consider 70-30 split

ames_train <- ames[index,]   # training data

ames_test <- ames[-index,]   # test data
```


EDA and data splitting has been done in the previous slides.

```{r}
# create recipe and blueprint, prepare and apply blueprint

set.seed(051624)   # set seed

ames_recipe <- recipe(Sale_Price ~ ., data = ames_train)   # set up recipe

blueprint <- ames_recipe %>%
  step_impute_mean(Gr_Liv_Area)                                   # impute missing entries


prepare <- prep(blueprint, data = ames_train)    # estimate feature engineering parameters based on training data


baked_train <- bake(prepare, new_data = ames_train)   # apply the blueprint to training data

baked_test <- bake(prepare, new_data = ames_test)    # apply the blueprint to test data
```


## Gradient Boosting: Implementation  {.smaller}

**Ames Housing Dataset**

```{r}
set.seed(051624)   # set seed

cv_specs <- trainControl(method = "repeatedcv", number = 5, repeats = 1)   # CV specifications

library(gbm)

out <- capture.output(
  gbm_cv <- train(blueprint,
                  data = ames_train,
                  method = "gbm",  
                  trControl = cv_specs,
                  tuneLength = 10,
                  metric = "RMSE")
  )
```

```{r}
gbm_cv$bestTune   # optimal tuning parameters

min(gbm_cv$results$RMSE)   # optimal CV RMSE
```


## Gradient Boosting: Implementation

**Ames Housing Dataset**

```{r}
# fit final model

final_model <- gbm(formula = Sale_Price ~ .,
                   data = baked_train,
                   n.trees = gbm_cv$bestTune$n.trees,
                   interaction.depth = gbm_cv$bestTune$interaction.depth,
                   n.minobsinnode = gbm_cv$bestTune$n.minobsinnode,
                   shrinkage = gbm_cv$bestTune$shrinkage)
```

```{r}
# obtain predictions on the test set

final_model_preds <- predict(object = final_model, newdata = baked_test, type = "response")  # test set predictions

sqrt(mean((final_model_preds - baked_test$Sale_Price)^2))   # test set RMSE
```


## Gradient Boosting: Implementation

```{r, fig.align='center'}
vip(final_model, num_features = 15)
```


## Multi-Class Classification: Iris Dataset 

We will work with the `iris` dataset which contains measurements in centimeters of four variables for 50 flowers from each of 3 species of iris: setosa, versicolor, and virginica. Please load the dataset using the following code. 

```{r, message=FALSE}
data(iris)        # load dataset
```

We are interested in predicting `Species` using the rest of the variables in the dataset. Note that this is a **multi-class classification** problem where our response has 3 classes/categories.


## Multi-Class Classification: Iris Dataset 

Let's investigate the dataset.

```{r}
glimpse(iris)   # all features are numerical
```

```{r}
summary(iris)   # summary of variables
```



## Multi-Class Classification: Iris Dataset 

Let's investigate the dataset.


```{r}
sum(is.na(iris))  # no missing entries
```

```{r}
nearZeroVar(iris, saveMetrics = TRUE)  # no zv/nzv features
```

We do not need to write a blueprint for implementing tree-based methods since trees don't require much feature engineering and there are no missing entries in the dataset. 

Note, if you are implementing KNN you would want to scale the features (using `step_center` and `step_scale`)


## Multi-Class Classification: Iris Dataset 

Data splitting

```{r}
# split the dataset

set.seed(051624)   # set seed

# split the data into training and test sets

index <- createDataPartition(iris$Species, p = 0.8, list = FALSE)

iris_train <- iris[index, ]

iris_test <- iris[-index, ]
```


## Multi-Class Classification: Iris Dataset

```{r}
cv_specs <- trainControl(method = "repeatedcv", number = 10, repeats = 5)   # CV specifications
```

```{r, eval = FALSE}
set.seed(051624)   # set seed

# CV with logistic regression

logistic_cv <- train(Species ~ .,
                     data = iris_train,
                     method = "glm",
                     family = "binomial",
                     trControl = cv_specs,
                     metric = "Accuracy")
```


The code above will throw an error since this is a 3-class classification problem and logistic regression (with family = binomial) works for a binary (2-class) problem.


## Multi-Class Classification: Iris Dataset

```{r}
set.seed(051624)   # set seed

# CV with gradient boosting

library(gbm)

out <- capture.output(
  iris_gbm_cv <- train(Species ~ .,
                       data = iris_train,
                       method = "gbm",  
                       trControl = cv_specs,
                       tuneLength = 5,
                       metric = "Accuracy")
  )
```

```{r}
iris_gbm_cv$bestTune   # optimal tuning parameters
```

```{r}
max(iris_gbm_cv$results$Accuracy)   # optimal CV Accuracy
```


## Multi-Class Classification: Iris Dataset

```{r}
# final model

gbm_fit <- gbm(formula = Species ~ .,
               data = iris_train,
               n.trees = iris_gbm_cv$bestTune$n.trees,
               interaction.depth = iris_gbm_cv$bestTune$interaction.depth,
               n.minobsinnode = iris_gbm_cv$bestTune$n.minobsinnode,
               shrinkage = iris_gbm_cv$bestTune$shrinkage)
```

We will need to write a bit of code to convert the probability predictions into class label predictions. The predicted class is the label with the highest (maximum) probability out of the 3 classes. 

```{r}
# probability predictions for each class

final_model_prob_preds <- matrix(predict(object = gbm_fit, newdata = iris_test, type = "response"),
                                 nrow = length(iris_test$Species),
                                 ncol = n_distinct(iris_test$Species))



# class label predictions for each class

final_model_class_preds <- factor(levels(iris_test$Species)[apply(final_model_prob_preds, 1, which.max)])
```


## Multi-Class Classification: Iris Dataset {.smaller}

```{r}
# confusion matrix

confusionMatrix(data = final_model_class_preds, reference = iris_test$Species)
```


